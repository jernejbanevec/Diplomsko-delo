% POZOR! - Pazi, da imaš nastavljeno vrednost "pdfLaTeX" v zgornjem okencu
\documentclass[mat1]{article}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{izrek}{Izrek}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}

%za x piši \times
% \in \mathbb{R}^{m \times n}

% Kjer so nejasnosti, dodaj znak oz. opozorilo za dopolnitev ali nejasnost
% \textbf{-- NEJASNOST!}
% \textbf{-- DOPOLNI!}

\begin{document}

\section{Uvod}

\subsection{Abstraktni uvod}
Diskriminantna analiza se že dolga leta uporablja za določevanje lastnosti, ki ohranjajo razlike med razredi. Definirana je kot optimizacijski problem, ki vključuje kovariančne matrike, ki predstavljajo razprešenost podatkov znotraj posameznega razreda in razpršenost oziroma ločenost posameznih razredov. Diskriminantna analiza pa sama po sebi zahteva, da je ena od teh kovariančnih matrik nesingularna, kar omejuje njeno uporabo na matrikah določenih dimenzij. V nadaljevanju tako preučimo več različnih optimizacijskih kriterijev in poskušamo njihovo uporabo razširiti na vse matrike z uporabo posplošenega singularnega razcepa. Na ta način se izognemo pogoju nesingularnosti, ki ga zahteva diskriminantna analiza. Na ta način dobimo posplošeno diskriminantno analizo, ki jo lahko uporabimo tudi kadar je ena matrika nesingularna (v nadaljevanju lahko vidimo, da je matrika nesingularna kadar je velikost vzorca manjša kot pa dimenzija posamezne meritve  \textbf{-- NEJASNOST!}). V nadaljevanju bom testiral učinkovitost posplošene diskriminantne analize in jo, kjer bo to mogoče, primerjal tudi z diskriminantno analizo.

\subsection{Matematični uvod}
Cilj diskriminantne analize je združevati lastnosti originalnih podatkov na način, ki kar najučinkoviteje ločuje med razredi, v katerih so podatki. Pri takšnem združevanju lastnosti podatkov se dimenzija teh podatkov zmanjša na način, ki najbolje ohranja strukture določenih razredov.  

Tu predpostavimo, da so podatki zloženi v matiko $A \in \mathbb{R}^{m \times n}$, kjer $m$ predstavlja dimenzijo posamezne meritve, $n$ pa predstavlja število meritev oz. podatkov. Denimo, da so podatki v matriki $A$ iz $k$ različnih razredov. Tako so stolpci $a_1, a_2, \ldots, a_n$ matrike $A$ združeni v $k$ podmatrik, ki predstavljajo razrede, v katerih so podatki:
$$ A = 
\begin{pmatrix}
A_1, & A_2, & \ldots, & A_k
\end{pmatrix} \text{,}
\hspace{2mm} \text{kjer} \hspace{2mm} A_i \in \mathbb{R}^{m \times n_i} \text{.}
$$ 

Cilj diskriminantne analize najti preslikavo $G^T$, ki v novem, manjdimenzionalnem prostoru, kar najbolje ohranja razrede, v katerih so podatki.
Za preslikavo $G^T$ torej velja: $$G^T : \mathbb{R}^m \rightarrow \mathbb{R}^\ell ,$$ kjer je $\ell \leq m - 1$. Torej preslikava $G^T$ nek $m$-dimenzionalen vektor preslika v nov vektor v $\ell$-dimezionalnem prostoru (navadno velja $\ell \leq m$), v katerem so razredi podatkov ohranjeni, razpršenost podatkov znotraj razredov je zmanjšana, razlike med razredi pa so povečane.

Tu število $n_i$ predstavlja moč indeksne množice razreda $i$. To indeksno množico razreda $i$ označujemo z $N_i$. Očitno velja tudi: $$
\sum_{i=1}^{k}n_i = n \text{.}$$

Za nadaljnje izračune moramo definirati tudi centroid $i$-tega razreda, ki je izračunan kot povprečje stolpcev v $i$-tem razredu, torej: 
$$c^{(i)} = \frac{1}{n_i} \sum_{j \in N_i} a_j
$$
in centroid celotnih podatkov, ki je izračunan kot povprečje vseh stolpcev:
$$c = \frac{1}{n} \sum_{j = 1}^{n} a_j \text{.}
$$

Razpršenost podatkov v razredih, razpršenost vseh podatkov ter razpršenost oziroma razlike med razredi je smiselno predstaviti s pomočjo matrik. Zato v nadaljevanju definiramo matriko, ki predstavlja matriko razpršenosti podatkov znotraj razredov:
$$S_W = \sum_{i = 1}^{k} \sum_{j \in N_i}(a_j - c^{(i)})(a_j - c^{(i)})^T\text{,}$$
matriko, ki predstavlja matriko razpršenosti oz razlik med razred:
$$S_B = \sum_{i = 1}^{k} \sum_{j \in N_i} ( c^{(i)} - c)( c^{(i)} - c)^T = \sum_{i = 1}^{k} n_i ( c^{(i)} - c)( c^{(i)} - c)^T$$
in matriko celotne razpršenosti podatkov:
$$S_M = \sum_{j = 1}^{n} (a_j - c)(a_j - c)^T$$

Med zgoraj definiranimi matrikami velja tudi enakost:
$S_M = S_W + S_B \text{.}$ \textbf{-- DOKAŽI}

S pomočjo preslikave $G^T$ preslikamo v $\ell$-dimezionalen prostor tudi matrike $S_W$, $S_B$ in $S_M$:
$$ S_{W}^{\ell} = G^T S_W G \text{,} \hspace{2mm} S_{B}^{\ell} = G^T S_B G \text{,} \hspace{2mm} S_{M}^{\ell} = G^T S_M G \text{.}
$$

Iz danih matrik razpršenosti podatkov bi radi tvorili kriterij kvalitete razredov. Kriterij kvalitete razredov bi imel visoko vrednost, kadar bi bili razredi, v katerih so podatki, strnjeni in dobro ločeni med seboj. Opazimo lahko, da $sled(S_W)$ predstavlja kako skupaj so si podatki v posameznem razredu, saj velja:
\begin{gather*} 
sled(S_W) = \sum_{t=1}^{m} (\sum_{i = 1}^{k} \sum_{j \in N_i}(a_{j_t} - c_t^{(i)})^2)
= \sum_{i = 1}^{k} \sum_{j \in N_i} ( \sum_{t=1}^{m} (a_{j_t} - c_t^{(i)})^2) \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ a_{j_t} - c_t^{(i)}}_2^2 \text{.}
\end{gather*}

Podobno $sled(S_B)$ predstavlja ločenost med razredi, saj velja:
\begin{gather*} 
sled(S_B) = \sum_{t=1}^{m} (\sum_{i = 1}^{k} \sum_{j \in N_i}(c_t^{(i)} - c_t)^2)
= \sum_{i = 1}^{k} \sum_{j \in N_i} ( \sum_{t=1}^{m} (c_t^{(i)} - c_t)^2) \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ c_t^{(i)} - c_t}_2^2 \text{.}
\end{gather*}

Optimalna preslikava $G^T$ tako maksimizira $sled(S_{B}^{\ell})$ in minimizira $sled(S_{W}^{\ell})$. Smiselen kriterij se tako zdi $$sled( G^T S_B G) / sled( G^T S_W G) \text{,}$$ ki pa ga zaradi težke izračunljivosti aproksimiramo kar z $$sled((S_W^\ell)^{-1}S_B^\ell) \text{.}$$
\textbf{-- NEJASNOST!}

Kljub temu, da je ta optimizacijski kriterij lažje izračunljiv ima svoje pomanjkljivosti. Opazimo lahko, da kriterij lahko uporabimo le v primeru, ko je matrika $S_W^\ell$ nesingularna oz. da kriterija nemorem uporabiti, ko je matrika $S_W^\ell$ singularna (torej kadar je njena determinanta enaka 0). Ker pa za determinanto matrike velja:
$$ det(S_W^\ell) = det(G^T S_W G) = det(G^T) det(S_W) det(G) \text{,}
$$
je $det(S_W^\ell)$ enaka 0 kadar je $det(S_W)$ enaka 0, torej kadar je matrika $S_W$ singularna. Do te situacije pa lahko pride kar precej pogosto. 


\section{Matematična priprava - posplošeni singularni razcep}
Originalna definicija posplošenega singularnega razcepa (Van Loan)
\begin{izrek}[Posplošeni singularni izrek (Van Loan):]
\label{izrek:SVD} Za matriki $K_A \in \mathbb{R}^{p \times m}$ z $p \geq m$ in $K_B \in \mathbb{R}^{n \times m}$ obstajata ortogonalni matriki $U \in \mathbb{R}^{p \times p}$ in $V \in \mathbb{R}^{n \times n}$ ter nesingularna matrika $X \in \mathbb{R}^{m \times m}$, da velja $$ U^T K_A X = diag(\alpha_1,..., \alpha_m) \ \text{in} \ V^T K_B X = diag(\beta_1,..., \beta_q) $$ kjer $q = min(n,m)$, $\alpha_i \geq 0$ za $1 \leq i \leq m$ in  $\beta_i \geq 0$ za  $1 \leq i \leq q$.
\end{izrek}

Problem tega izreka ja, da se ga ne da uporabiti, kadar dimenzije matrike $K_A$ niso ustrezne. Zaradi tega pretirano zavezujočega pogoja se odločita C.C. Paige in M.A. Saunders ta posplošeni singularni izrek še dodatno posplošiti. Tako dobimo naslednji izrek:
\begin{izrek}[Posplošeni singularni izrek (Paige in Saunders):]
\label{izrek:GSVD} Naj bosta dani matriki $K_A \in \mathbb{R}^{p \times m}$ in $K_B \in \mathbb{R}^{n \times m}$. Potem za $K = \left(\begin{array}{c} K_A \\ K_B \end{array}\right)$ in $t = rang(K)$ obstajajo ortogonalne matrike $U \in \mathbb{R}^{p \times p}$, $V \in \mathbb{R}^{n \times n}$, $W \in \mathbb{R}^{t \times t}$ in $Q \in \mathbb{R}^{m \times m}$, da velja: 
$$U^T K_A Q = \Sigma_A  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right) \hspace{4mm} \text{in} \hspace{4mm} V^T K_B Q = \Sigma_B  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right),$$ kjer je 
$$\Sigma_A = \begin{pmatrix} 
I_A &  & \\
 & D_A & \\
 & & 0_A  
\end{pmatrix} \hspace{4mm} \text{in} \hspace{4mm}
\Sigma_B = \begin{pmatrix} 
0_B &  & \\
 & D_B & \\
 & & I_B  
\end{pmatrix}.$$ 
$R \in \mathbb{R}^{t \times t}$ je nesingularna matrika, matriki $I_A \in \mathbb{R}^{r \times r}$ in $I_B \in \mathbb{R}^{(t-r-s) \times (t-r-s)}$ identični matriki, kjer je 
$$r = rang(K) - rang(K_B) \hspace{4mm} \text{in} \hspace{4mm} s = rang(K_A) + rang(K_B) - rang(K),$$
$0_A \in \mathbb{R}^{(p-r-s) \times (t-r-s)}$ in $0_B \in \mathbb{R}^{(n-t+r) \times r}$ ničelni matriki, ki imata lahko tudi ničelno število vrstic ali stolpcev, matriki
$D_A = diag(\alpha_{r+1},..., \alpha_{r+s})$ in $D_B = diag(\beta_{r+1},..., \beta_{r+s})$ pa diagonalni matriki, ki zadoščata pogoju:
$$1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0 \hspace{4mm} \text{in} \hspace{4mm} 0 < \beta_{r+1} \leq \ldots \leq \beta_{r+s} < 1$$
pri $\alpha_i^2 + \beta_i^2 = 1$ za $ i = r+1,\ldots, r+s$
\end{izrek}
\begin{proof}
Dovolj je, če ta izrek dokažemo za vsa kompleksna števila. Iz dejstva, da je množica realnih števil ($\mathbb{R}$) podmnožica množice kompleksnih števil ($\mathbb{C}$), sledi, da potem ta izrek velja tudi za vsa realna števila. Definirajmo matriko $K$, ki je sestavljena kot matrika sestavljena iz $K_A$ in $K_B$, torej $$K:= \begin{pmatrix} 
K_A \\
K_B 
\end{pmatrix}.$$
Na zgoraj definirani matriki $K$ lahko sedaj naredimo singularni razcep. Tako vemo, da za matriko $K$ obstajata unitarni matriki $P \in \mathbb{C}^{(m+p) \times (m+p)}$ in $Q \in \mathbb{C}^{n \times n}$, da velja $$ P^H K Q = 
\begin{pmatrix} 
R & 0 \\
0 & 0 
\end{pmatrix}$$, kjer ima matrika $R$ enak rang kot matrika $K$. Matriki $Q$ in $P$ sedaj ločimo na sledeče podmatrike:
$$ Q = 
\begin{pmatrix} 
Q_1, & Q_2
\end{pmatrix}
\hspace{4mm} \text{in} \hspace{4mm}
P = 
\begin{pmatrix} 
P_1, & P_2
\end{pmatrix}
=
\begin{pmatrix} 
P_{11} & P_{12} \\
P_{21} & P_{22} 
\end{pmatrix},
$$
kjer je matrika $Q_1 \in \mathbb{C}^{m \times t}$ sestavljena iz prvih $k$ stolpcev matrike $Q$, matrika $P_1 \in \mathbb{C}^{(p+n) \times t}$ pa izprvih $t$ stolpcev matrike $P$ in njena podmatrika $P_{11} \in \mathbb{C}^{p \times t}$ pa iz prvih m vrstic matrike $P_1$.
Vemo, da ker je matrika $P$ unitarna matrika, velja $\norm{P}_2 \leq 1$ in posledično velja še $\norm{P_{11}}_2 \leq \norm{P_{1}}_2 \leq \norm{P}_2 \leq 1$. Iz izreka iz numeričnih metod velja, da posledično nobena lastna vrednost matrike $P_{11}$ ni večja od $1$.

Singularni razcep podobno kot na matriki $K$ naredimo tudi na matriki $P_{11}$. Tako dobimo takšni matriki $U \in \mathbb{C}^{p \times p}$ in $W \in \mathbb{C}^{ \times t}$, da velja $$ U^H P_{11} W = \Sigma_A,$$ kjer je $$\Sigma_A = 
\begin{pmatrix} 
0_B &  & \\
 & D_B & \\
 & & I_B  
\end{pmatrix},$$ kjer je matrika $D_B$ diagonalna matrika z diagonalnimi vrednostmi $\alpha_{r+1},..., \alpha_{r+s},$ za katere velja $1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0.$

(Spodnji del je vprašljiv? \textbf{-- DOPOLNI!})

Na matriki $P_{21}$ uporabimo čudežni razcep (s Householderjevimi zrcaljenji) in tako dobimo matriko $V \in \mathbb{C}^{n \times n}$, da velja $$ V^H P_{21} W = L = (\ell_{ij})_{i, j} =
\begin{pmatrix} 
0 &  \\
& L_1
\end{pmatrix}
,$$ kjer je matrika $L_1$ spodnjetrikotna z diagonalnimi elementi večjimi od $0$.
Opazimo lahko, da velja spodnja enakost
$$
\begin{pmatrix} 
U^T & 0 \\
0 & V^T
\end{pmatrix}
\begin{bmatrix}
P_{11} \\
P_{21}
\end{bmatrix} W = 
\begin{bmatrix}
U^H P_{11} W \\
V^H P_{21} W
\end{bmatrix} =
\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}.
$$
Zgornja matrika $\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}$ je unitarna, saj je produkt unitarnih matrik. Posledično so njeni stolpci ortonormirani.
\end{proof}


% DODATNO - IZPIS MATIK Z DIMENZIJAMI
%\begin{tikzpicture}[decoration=brace]
%    \matrix (m) [matrix of math nodes,left delimiter=[,right delimiter={]}] {
%        a & b & c & d \\
%        e & f & g & h \\
%        i & j & k & l \\
%        m & n & o & p \\
%    };
%    \draw[decorate,transform canvas={xshift=-1.5em},thick] (m-3-1.south west) -- node[left=2pt] {$A$} (m-1-1.north west);
%    \draw[decorate,transform canvas={yshift=0.5em},thick] (m-1-2.north west) -- node[above=2pt] {$R$} (m-1-4.north east);
%\end{tikzpicture}

\section{Matematična rešitev problema}
\subsection{Posplošitev linearne diskriminantne analize}

\subsection{Posplošitev maksimizacijskega kriterija $sled((S_W^Y)^{-1}S_B^Y)$}

\section{Algoritem}

\section{Zaključek}

\section{Viri}

\end{document}