% POZOR! - Pazi, da imaš nastavljeno vrednost "pdfLaTeX" v zgornjem okencu
\documentclass[mat1]{article}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{arydshln}
%\usepackage{fontspec,xcolor}
%\usepackage{luacode,luatexbase}
\newtheorem{izrek}{Izrek}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}

%za x piši \times
% \in \mathbb{R}^{m \times n}

% Kjer so nejasnosti, dodaj znak oz. opozorilo za dopolnitev ali nejasnost
% \textbf{-- NEJASNOST!}
% \textbf{-- DOPOLNI!}

%\begin{luacode}
%local function vartosrcvar ( line )
%  return string.gsub(line, " var " , " \\src{var} ") 
%end
%luatexbase.add_to_callback( "process_input_buffer",  vartosrcvar, "var_to_srcvar")
%\end{luacode}

\begin{document}

\section{Uvod}

\subsection{Uvod}

\textbf{-- Ta odstavek je potrebno dopolniti!}

Diskriminantna analiza se že dolga leta uporablja za določevanje lastnosti, ki poudarjajo razlike med razredi. Definirana je kot optimizacijski problem, ki vključuje kovariančne matrike, ki predstavljajo razpršenost podatkov znotraj posameznega razreda in razpršenost oziroma ločenost posameznih razredov. Diskriminantna analiza pa sama po sebi zahteva, da je ena od teh kovariančnih matrik nesingularna, kar omejuje njeno uporabo na matrikah določenih dimenzij. V nadaljevanju tako preučimo več različnih optimizacijskih kriterijev in poskušamo njihovo uporabo razširiti na vse matrike z uporabo posplošenega singularnega razcepa. Na ta način se izognemo pogoju nesingularnosti, ki ga zahteva diskriminantna analiza. Tako pridemo do posplošene diskriminantne analize, ki jo lahko uporabimo tudi, kadar je ena matrika singularna (v nadaljevanju lahko vidimo, da je matrika singularna, kadar je velikost vzorca manjša, kot pa je dimenzija posamezne meritve. V nadaljevanju bomo testirali učinkovitost posplošene diskriminantne analize in jo, kjer bo to mogoče, primerjali tudi z običajno diskriminantno analizo.

\subsection{Matematični uvod}

Cilj diskriminantne analize je združiti lastnosti originalnih podatkov na način, ki kar najučinkoviteje ločuje med razredi, v katerih so podatki. Pri takšnem združevanju lastnosti podatkov se dimenzija teh podatkov zmanjša tako, da se struktura teh podatkov in določenih razredov kar najbolje ohrani.

Predpostavimo, da so podatki zloženi v matriko $A \in \mathbb{R}^{m \times n}$, kjer $m$ predstavlja dimenzijo posamezne meritve, $n$ pa predstavlja število meritev oz. podatkov. Denimo, da so podatki v matriki $A$ iz $k$ različnih razredov. Tako so stolpci $a_1, a_2, \ldots, a_n$ matrike $A$ združeni v $k$ podmatrik, ki predstavljajo razrede, v katerih so podatki:
$$ A = 
\begin{bmatrix}
A_1, & A_2, & \ldots, & A_k
\end{bmatrix} \text{,}
\hspace{2mm} \text{kjer} \hspace{2mm} A_i \in \mathbb{R}^{m \times n_i} \text{.}
$$ 
Tu število $n_i$ predstavlja moč indeksne množice razreda $i$. To indeksno množico razreda $i$ označimo z $N_i$. Očitno velja tudi $$
\sum_{i=1}^{k}n_i = n \text{.}$$
Matriko $A$ lahko poleg razdelitve na podmatrike razdelimo tudi na stolpce. Matrika $A = \left[ a_{i ,j} \right]  \in \mathbb{R}^{m \times n}$ je tako sestavljena iz $n$ posameznih stolpcev, kjer $i$-ti stolpec označimo z $a_i$:
$$ a_i =
\begin{bmatrix}
a_{1, i} \\
a_{2, i} \\
\vdots \\
a_{m, i}
\end{bmatrix}
\text{.}
$$

Cilj diskriminantne analize je najti linearno preslikavo iz $\mathbb{R}^m$ v $\mathbb{R}^\ell$, ki v novem prostoru kar najbolje poudari razrede, v katerih so podatki. Tu navadno velja $\ell \leq m - 1$, torej da je prostor, v katerega ta linearna preslikava slika, manjdimenzionalen kot prvotni prostor. To iskano linearno preslikavo predstavimo z matriko $G^T \in \mathbb{R}^{\ell \times m}$.
Za preslikavo $G^T$ torej velja $$G^T : \mathbb{R}^m \rightarrow \mathbb{R}^\ell \text{.}$$ Torej preslikava $G^T$ nek $m$-dimenzionalen vektor preslika v nov vektor v $\ell$-dimezionalnem prostoru, v katerem so razredi podatkov poudarjeni, razpršenost podatkov znotraj razredov je zmanjšana, razlike med razredi pa so povečane.

Za nadaljnje izračune moramo definirati tudi centroid $i$-tega razreda, ki je izračunan kot povprečje stolpcev v $i$-tem razredu, 
$$c^{(i)} = \frac{1}{n_i} \sum_{j \in N_i} a_j \text{,}
$$
in centroid celotnih podatkov, ki je izračunan kot povprečje vseh stolpcev, to je
$$c = \frac{1}{n} \sum_{j = 1}^{n} a_j \text{.}
$$

Razpršenost podatkov znotraj posameznih razredov, razpršenost vseh podatkov ter razpršenost oziroma razlike med razredi je smiselno predstaviti s pomočjo matrik. Zato v nadaljevanju definiramo matriko
$$S_W = \sum_{i = 1}^{k} \sum_{j \in N_i}(a_j - c^{(i)})(a_j - c^{(i)})^T\text{,}$$
ki predstavlja matriko razpršenosti podatkov znotraj razredov, matriko
$$S_B = \sum_{i = 1}^{k} \sum_{j \in N_i} ( c^{(i)} - c)( c^{(i)} - c)^T = \sum_{i = 1}^{k} n_i ( c^{(i)} - c)( c^{(i)} - c)^T \text{,}$$
ki predstavlja matriko razpršenosti oziroma razlik med razredi in matriko
$$S_M = \sum_{j = 1}^{n} (a_j - c)(a_j - c)^T \text{,}$$
ki pradstavlja matriko celotne razpršenosti podatkov. Vse tri matrike so velikosti $m \times m$.
S pomočjo preslikave $G^T$ pa jih preslikamo v matrike velikosti $\ell \times \ell$ na sledeč način:
$$ S_{W}^{\ell} = G^T S_W G \text{,} \hspace{2mm} S_{B}^{\ell} = G^T S_B G \text{,} \hspace{2mm} S_{M}^{\ell} = G^T S_M G \text{.}
$$
%Med zgoraj definiranimi matrikami velja tudi enakost:
%$S_M = S_W + S_B \text{.}$ \textbf{-- DOKAŽI (pozneje, ko vpelješ še matrike H)}

Iz danih matrik razpršenosti podatkov bi radi tvorili kriterij kvalitete strukture razredov. Kriterij kvalitete strukture razredov mora imeti visoko vrednost, kadar so razredi, v katerih so podatki, strnjeni in dobro ločeni med seboj. Opazimo lahko, da $sled(S_W)$ predstavlja, kako skupaj so si podatki v posameznem razredu, saj velja
\begin{gather*} 
sled(S_W) = \sum_{t=1}^{m} \left[ \sum_{i = 1}^{k} \sum_{j \in N_i}(a_{t, j} - c_t^{(i)})^2 \right]
= \sum_{i = 1}^{k} \sum_{j \in N_i} \left[ \sum_{t=1}^{m} (a_{t, j} - c_t^{(i)})^2 \right] \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ a_j - c^{(i)}}_2^2 \text{.}
\end{gather*}
Podobno $sled(S_B)$ predstavlja ločenost med razredi, saj velja
\begin{gather*} 
sled(S_B) = \sum_{t=1}^{m} \left[ \sum_{i = 1}^{k} \sum_{j \in N_i}(c_t^{(i)} - c_t)^2\right]
= \sum_{i = 1}^{k} \sum_{j \in N_i} \left[ \sum_{t=1}^{m} (c_t^{(i)} - c_t)^2 \right] \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ c^{(i)} - c}_2^2 
= \sum_{i = 1}^{k} n_i \norm{ c^{(i)} - c}_2^2
\text{.}
\end{gather*}

Optimalna preslikava $G^T$ tako maksimizira $sled(S_{B}^{\ell})$ in minimizira $sled(S_{W}^{\ell})$. Smiselen maksimizacijski kriterij se tako zdi $$sled( G^T S_B G) / sled( G^T S_W G) \text{,}$$ ki pa ga zaradi lažjega računanja aproksimiramo kar s kriterijem $$sled((S_W^\ell)^{-1}S_B^\ell) \text{.}$$
%\textbf{-- NEJASNOST!}
% Pojasnitev prvega kriterija: Preslikana matrika S_B mora imeti čim manjšo vsoto lastnih vrednosti, preslikana matrika S_W pa čim večjo.
% Pojasnitev aproksimacijskega kriterija: Skalarni produkt lastnih vrednosti preslikanih matrik mora biti čim manjši - to je pribljižno enako

Kljub temu, da je ta optimizacijski kriterij lažje izračunljiv ima svoje pomanjkljivosti. Opazimo lahko, da kriterija ne moremo uporabiti, ko je matrika $S_W^\ell$ singularna, torej kadar je njena determinanta enaka 0. 

\textbf{-- *Ali ta del sploh velja?*--}

\textbf{--Će NE, kako lahko potem pokažem da singularni razcep res potrebujemo? --}

\textbf{--Će DA, kako lahko to dokažem/pokažem? --}
%Ta del mogoče sploh ne velja!!

Iz singularnosti matrike $S_W$ sledi singularnoti matrike $S_W^\ell$.
%Ker pa za determinanto matrike velja
%$$ \det(S_W^\ell) = \det(G^T S_W G) = \det(G^T) \cdot \det(S_W) \cdot \det(G) \text{,}
%$$
%je $\det(S_W^\ell)$ enaka 0 kadar je $\det(S_W)$ enaka 0, torej kadar je matrika $S_W$ singularna. 

\textbf{--}

Do te situacije pa lahko pride kar precej pogosto. Matrika $S_W \in  \mathbb{R}^{m \times m}$ je singularna namreč v vseh primerih, ko za matriko $A \in  \mathbb{R}^{m \times n}$ velja $m > n$, saj je potem po definiciji sestavljena kot vsota $n$ matrik z rangom $1$. $m \times m$ dimenzionalna matrika $S_W$ ima tako rang manjši ali enak $n$, iz česar sledi, da je njena determinanta enaka 0.
Na primer, do tega problema pride v primeru, ko je pridobivanje podatkov drago oz. zahtevno in so pridobljeni podatki visokih dimenzij (dimenzija posameznega podatka je večja od števila vseh pridobljenih podatkov).

Obstaja več načinov, kako aplicirati diskriminantno analizo na matriki $A \in \mathbb{R}^{m \times n}$ z $m > n$. V grobem jih ločimo na tiste, kjer dimenzijo podatkov zmanjšamo v dveh korakih, in na tiste, kjer dimenzijo podatkov zmanjšamo v enem koraku. Pri prvem načinu se faza diskriminante analize nadaljuje v fazo, v kateri zanemarimo oblike posameznih razredov. Najpopularnejša metoda za prvi del tega procesa je zmanjšanje ranga matrike s pomočjo singularnega razcepa. To je tudi glavno orodje metode imenovane metoda glavnih komponent. Kakorkoli, celotna predstava dvostopenjskih načinov je precej občutljiva na zmanjšanje dimenzije v prvi fazi. V diplomskem delu se bomo osredotočili na način, ki posploši diskriminantno analizo tako, da teoretično optimalno zmanjša dimenzijo podatkov brez da bi uvedel dodaten korak. V ta namen bomo obravnavali kriterij 
$$ sled((S_2^\ell)^{-1} S_1^\ell) \hspace{1mm} \text{,}
$$
kjer matriki $S_2$ in $S_1$ predstavljata poljubno matriko izmed $S_W$, $S_B$ in $S_M$.  Kadar je matrika $S_2$ nesingularna, klasična diskriminantna analiza predstavi svojo rešitev s pomočjo posplošenega problema lastnih vrednosti. S prestrukturiranjem problema tako, da uporabimo posplošeni singularni razcep, pa lahko razširimo uporabnost diskriminantne analize tudi na primer, ko je matrika $S_2$ singularna.

\section{Matematična priprava - posplošeni singularni razcep}
Originalna definicija posplošenega singularnega razcepa (Van Loan) je sledeča.

% ZAČETEK PRVEGA IZREKA
\begin{izrek}[Posplošeni singularni izrek (Van Loan)]
\label{izrek:SVD} Za matriki $K_A \in \mathbb{R}^{p \times m}$ z $p \geq m$ in $K_B \in \mathbb{R}^{n \times m}$ obstajata ortogonalni matriki $U \in \mathbb{R}^{p \times p}$ in $V \in \mathbb{R}^{n \times n}$ ter nesingularna matrika $X \in \mathbb{R}^{m \times m}$, da velja 
$$ U^T K_A X = 
% !!DEFINICIJA PRVE MATRIKE!!
\begin{bmatrix}
\begin{matrix}
\alpha_1 & & \\
 & \ddots & \\
 & & \alpha_m
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p-m, m}
\end{bmatrix} 
% !!KONEC DEFINICIJE PRVE MATRIKE!!
\hspace{2mm} \text{in} \hspace{2mm}
 V^T K_B X = 
\Sigma_{B_q} \text{,}
$$ kjer je $q = min(n,m) \text{,}$
% !!DEFINICIJA DRUGE MATRIKE!!
\begin{gather*}
\Sigma_{B_q} = 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_q
\end{matrix} & 0_{q, m - q}
 \\ \hdashline[2pt/2pt]
0_{n-q, q} & 0_{n-q, m-q}
\end{array} \right] \text{,} 
\end{gather*} %\hspace{2mm}
% !!KONEC DEFINICIJE DRUGE MATRIKE!!
$
\alpha_i \geq 0 \hspace{2mm} \text{za} \hspace{2mm}
 1 \leq i \leq m \text{,} \hspace{2mm}
  \beta_i \geq 0 \hspace{2mm} \text{za} \hspace{2mm} 1 \leq i \leq q
\hspace{2mm} \text{in} \hspace{2mm} \beta_1 \geq \beta_2 \geq \ldots \geq \beta_q
\text{.}
$
\end{izrek}
\begin{proof}
% PREVERI SPODNJE PRETVORBE
% m_a = p
% m_b = n
% k = n
% k_1 = k
% n = m

Iz matrik $K_A$ in $K_B$ tvorimo združeno $(p+n)\times m$ matriko $K = \left[\begin{array}{c} K_A \\ K_B \end{array}\right]$, na kateri naredimo singularni razcep. Iz singularnega razcepa dobimo matriki $Q \in \mathbb{R}^{(p+n) \times (p+n)}$ in matriko $Z_1 \in \mathbb{R}^{m \times m}$, tako da velja 
\begin{equation}
Q^T \left[\begin{array}{c} K_A \\ K_B \end{array}\right] Z_1 = 
\begin{bmatrix}
\begin{matrix}
\gamma_1 & & \\
 & \ddots & \\
 & & \gamma_m
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p+n-m, m}
\end{bmatrix} 
\text{,}
\label{eq:1}
\end{equation}
kjer za
$
k = rang(K) \hspace{2mm} \text{velja} \hspace{2mm} 
\gamma_1 \geq \ldots \geq \gamma_k > \gamma_{k+1} = \ldots \gamma_m = 0\text{.} $

Matriko $Z_1$ razdelimo na dve matriki, matriko $Z_{11} \in \mathbb{R}^{m \times k}$, ki je sestavljena iz prvih $k$ stolpcev matrike $Z_1$ in matriko $Z_{12} \in \mathbb{R}^{m \times (m-k)}$, ki je sestavljena iz preostalih $m-k$ stolpcev matrike $Z_1$. Pišemo $$Z_1 = \left[ Z_{11} \hspace{2mm} Z_{12} \right] \text{.}$$

%, lahko vidimo, da velja
%$$ Q^T K \left[\begin{array}{c} Z_{11} | Z_{12} \end{array}\right] = 
%\begin{bmatrix}
%\begin{matrix}
%\gamma_1 & & & & & \\
% & \ddots & & & & \\
% & & \gamma_k & & & \\
% & & & \gamma_{k+1} & & \\
% & & & & \ddots & \\
% & & & & & \gamma_m 
%\end{matrix} \\ \hdashline[2pt/2pt]
%0_{p+n-m, m}
%\end{bmatrix}
%\text{.}
%$$
Po predpostavki velja $p \geq m$ in ker je očitno tudi $m \geq k$ sledi $p \geq m \geq k$. Sedaj definirajmo matriko 
$$ D := diag(\gamma_1,..., \gamma_k) \in \mathbb{R}^{k \times k} \text{.}
$$
Tako iz zgornje enačbe \textit{\eqref{eq:1}} dobimo
\begin{equation}
\begin{bmatrix}
K_A Z_{11} & K_A Z_{12} \\ 
K_B Z_{11} & K_B Z_{12}
\end{bmatrix} = Q
\begin{bmatrix}
D & 0_{k, m-k} \\ 
0_{p+n-k, k} & 0_{p+n-k, m-k} 
\end{bmatrix} \text{,} \label{eq:2}
\end{equation}
od koder sledi
$$
\begin{bmatrix}
K_A Z_{11} \\ 
K_B Z_{11}
\end{bmatrix} = Q
\begin{bmatrix}
D \\ 
0
\end{bmatrix} \text{.}
$$
V kolikor še matriko $Q$ razdelimo na podmatrike na naslednji način
$$ Q = 
\begin{bmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{bmatrix} \text{,}
$$ kjer je matrika $Q_{11} \in \mathbb{R}^{p \times k}$, matrika $Q_{12} \in \mathbb{R}^{p \times (p+n-k)}$, matrika $Q_{21} \in \mathbb{R}^{ n \times k}$ in matrika matrika $Q_{22} \in \mathbb{R}^{ n \times (p+n-k)}$, ugotovimo, da je
$$Q
\begin{bmatrix}
D \\ 
0
\end{bmatrix} = 
\begin{bmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{bmatrix}
\begin{bmatrix}
D \\ 
0
\end{bmatrix} =
\begin{bmatrix}
Q_{11} D \\ 
Q_{21} D
\end{bmatrix} \text{.}
$$
Iz tega neposredno sledi enakost
$$
K_A Z_{11} = Q_{11} D \implies K_A Z_{11} D^{-1} = Q_{11} =: K_{A_1}  \in \mathbb{R}^{p \times k} %\text{,}
$$
in enakost
$$
K_B Z_{11} = Q_{21} D \implies K_B Z_{11} D^{-1} = Q_{21} =: K_{B_1} \in \mathbb{R}^{ n \times k} \text{.}
$$
Sedaj singularni razcep naredimo na matriki $K_{B_1}$. Za matriko $K_{B_1}$ vemo, da ima isti rang kot matrika $K_B$, saj velja, da je matrika $Z_{11}$ polnega ranga (je namreč podmatrika ortogonalne matrike $Z$) in vemo, da je matrika $D^{-1}$ polnega ranga. Označimo $r = rang(K_B) = rang(K_{B_1})$. Iz singularnega razcepa za matriko $K_{B_1}$ dobimo ortogonalni matriki $V \in \mathbb{R}^{ n \times n}$ in $Z_2 \in \mathbb{R}^{ k \times k}$, da velja
\begin{equation}
V^T K_{B_1} Z_2 = \Sigma_{B_t}
 \text{,}  \label{eq:3}
\end{equation}
kjer je $t = min\{n, k\}$, 
$\Sigma_{B_t} = 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_t
\end{matrix} & 0_{t, k - t}
 \\ \hdashline[2pt/2pt]
0_{n-t, t} & 0_{n-t, k-t}
\end{array} \right]$
in velja 
$ \beta_1 \geq \beta_2 \geq \ldots \geq \beta_r > \beta_{r+1} = \ldots = \beta_t = 0 \text{.}$ 
\newline
Iz enačbe \textit{\eqref{eq:2}} sledi, da je
$$
K_B Z_{12} = 0_{n, m-k} \text{.}
$$
Opazimo, da velja tudi
\begin{gather*}
V^T K_B Z_1
\begin{bmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{bmatrix} = 
V^T K_B \left[ Z_{11} \hspace{2mm} Z_{12} \right]
\begin{bmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{bmatrix} = \\
V^T K_B
\begin{bmatrix}
 Z_{11} D^{-1} Z_2 & Z_{12} 
\end{bmatrix} =
\begin{bmatrix}
V^T K_B Z_{11} D^{-1} Z_2 & V^T K_B Z_{12} 
\end{bmatrix} = \\
\begin{bmatrix}
V^T K_{B_1} Z_2 & 0_{n, m-k} 
\end{bmatrix} =
\begin{bmatrix}
\Sigma_{B_t} & 0_{n, m-k} 
\end{bmatrix} = \\
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_t
\end{matrix} & 0_{t, m - t}
 \\ \hdashline[2pt/2pt]
0_{n-t, t} & 0_{n-t, m-t}
\end{array} \right]
\text{.}
\end{gather*}
Če za $q = \min\{n,m\}$ dodatno definiramo še $\beta_{t+1} = \ldots = \beta_{q} = 0$, dobimo ravno
\begin{gather*}
V^T K_B Z_1
\begin{bmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{bmatrix} = 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_q
\end{matrix} & 0_{q, m - q}
 \\ \hdashline[2pt/2pt]
0_{n-q, q} & 0_{n-q, m-q}
\end{array} \right]
\text{,}
\end{gather*}
kar je pa ravno matrika $\Sigma_{B_q}$ iz izreka. Matriko $X$ tako definiramo na sledeči način
$$ X := Z_1 
\begin{bmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{bmatrix}
\text{.}$$

Dokazati pa moramo tudi, da zgornja matrika $X$ ustreza tudi enačbi iz izreka za matriko $K_A$, torej, da obstaja tudi taka matrika $U$, da velja
\begin{gather*}
 U^T K_A X = 
\begin{bmatrix}
\begin{matrix}
\alpha_1 & & \\
 & \ddots & \\
 & & \alpha_m
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p-m, m}
\end{bmatrix}
\text{.} 
\end{gather*}

Ker je matrika $Q$ ortogonalna, dodatno velja $K_{A_1}^TK_{A_1} + K_{B_1}^TK_{B_1} = I_k$, kjer je $I_k$ identična matrika dimenzije $k \times k$. To enakost lahko pokažemo tako, da razpišemo spodnjo enačbo
\begin{gather*}
Q^T Q = 
\begin{bmatrix}
Q_{11}^T & Q_{21}^T \\ 
Q_{12}^T & Q_{22}^T
\end{bmatrix}
\begin{bmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{bmatrix} =
\begin{bmatrix}
Q_{11}^T Q_{11} + Q_{21}^T Q_{21} & Q_{11}^T Q_{12} + Q_{21}^T Q_{22} \\ 
Q_{12}^T Q_{11} + Q_{22}^T Q_{21} & Q_{12}^T Q_{12} + Q_{22}^T Q_{22}
\end{bmatrix} \\ =
\begin{bmatrix}
K_{A_1}^TK_{A_1} + K_{B_1}^TK_{B_1} & Q_{11}^T Q_{12} + Q_{21}^T Q_{22} \\ 
Q_{12}^T Q_{11} + Q_{22}^T Q_{21} & Q_{12}^T Q_{12} + Q_{22}^T Q_{22}
\end{bmatrix}  = I =
\begin{bmatrix}
I_k & 0\\ 
0 & I_{p+n-k}
\end{bmatrix} \text{.}
\end{gather*}
Definirajmo matriko $G$, ki jo dobimo s preoblikovanjem enačbe \textit{\eqref{eq:3}}:
$$ G := K_{B_1} Z_2 = V \Sigma_{B_t} \in \mathbb{R}^{n \times k} \text{.}
$$

Za matriko $K_{A_1} Z_2$ izračunamo razširjen QR razcep, $ K_{A_1} Z_2 = U R$, kjer je $U \in \mathbb{R}^{p \times p}$ ortogonalna in $R \in \mathbb{R}^{p \times k}$ zgornja trapezna matrika. Tak razcep lahko naredimo na primer z uporabo Householderjevih zrcaljenj.

%STAR DEL!!!
%naredimo Householderjeva zrcaljenja: Dobimo matriko ortogonalno matriko $U^T \in \mathbb{R}^{p \times p}$, da velja
%$$ U^T K_{A_1} Z_2 = R \text{,}
%$$
%kjer je $R \in \mathbb{R}^{p \times k}$ matrika z zgornjo trapezno obliko. Ker je matrika $U$ ortogonalna, velja $ K_{A_1} Z_2 = U R$.

Opazimo lahko, da so stolpci matrike $K_{A_1} Z_2$ medsebojno ortogonalni, saj velja
\begin{gather*}
(K_{A_1} Z_2)^T (K_{A_1} Z_2) = Z_2^T K_{A_1}^T K_{A_1} Z_2 %\\
= Z_2^T (I_{k} - K_{B_1}^T K_{B_1}) Z_2 =
\\
Z_2^T Z_2 -  Z_2^T K_{B_1}^T K_{B_1} Z_2
= I_k - G^T G = I_k - \Sigma_{B_t}^T V^T V \Sigma_{B_t} 
\\
= I_k - 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1^2 & & \\
 & \ddots & \\
 & & \beta_t^2
\end{matrix} & 0_{t,k - t}
 \\ \hdashline[2pt/2pt]
0_{k - t, k - t} & 0_{t, k - t}
\end{array} \right] 
%\\
= 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
1 - \beta_1^2 & & \\
 & \ddots & \\
 & & 1 - \beta_t^2
\end{matrix} & 0_{t,k - t}
 \\ \hdashline[2pt/2pt]
0_{k - t, k - t} & I_{t, k - t}
\end{array} \right]  
\\ 
= diag(1-\beta_1^2, \ldots, 1-\beta_k^2) \text{,}
\end{gather*}
kjer smo dodatno definirali še $\beta_{t+1} = \ldots = \beta_{k} = 0$. 
Iz tega sledi, da je matrika $R$ oblike 
$$ R = 
\begin{bmatrix}
\begin{matrix}
\sqrt{1 - \beta_1^2} & & \\
 & \ddots & \\
 & & \sqrt{1 - \beta_k^2}
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p-k, k}
\end{bmatrix} \text{,}
$$ saj velja 
$$
(K_{A_1} Z_2)^T (K_{A_1} Z_2) = R^T U^T U R = R^T R = diag(1-\beta_1^2, \ldots, 1-\beta_k^2) \text{.}
$$
Velja tudi
\begin{gather*}
U^T K_A Z_1
\begin{bmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{bmatrix} = 
U^T K_A %\left[\begin{array}{c} Z_{11} | Z_{12} \end{array}\right]
\left[Z_{11} \hspace{2mm} Z_{12} \right]
\begin{bmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{bmatrix} = \\
U^T K_A
\begin{bmatrix}
 Z_{11} D^{-1} Z_2 & Z_{12} 
\end{bmatrix} =
\begin{bmatrix}
U^T K_A Z_{11} D^{-1} Z_2 & U^T K_A Z_{12} 
\end{bmatrix} = \\
\begin{bmatrix}
U^T K_{A_1} Z_2 & 0_{p, m-k} 
\end{bmatrix} =
\begin{bmatrix}
U^T U R & 0_{p, m-k}
\end{bmatrix} =
\begin{bmatrix}
R & 0_{p, m-k} 
\end{bmatrix} = \\
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\alpha_i & & \\
 & \ddots & \\
 & & \alpha_k
\end{matrix} & 0_{k, m-k}
 \\ \hdashline[2pt/2pt]
0_{p-k, k} & 0_{p-k, m-k}
\end{array} \right] \text{,}
\end{gather*}
kjer smo dodatno definirali $\alpha_i = \sqrt{1-\beta_i^2}$ za $i = 1, \ldots, k$ in $\alpha_{k+1} = \ldots = \alpha_m$.
S tem smo pokazali, da matrika $$ X = Z_1 
\begin{bmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{bmatrix}
$$
zadošča tako razcepu matrike $K_A$ kot tudi razcepu matrike $K_B$ iz izreka in s tem dokazali izrek.
\end{proof}

Problem tega izreka pa je, da se ga ne da uporabiti, kadar dimenzije matrike $K_A$ niso ustrezne. Zaradi tega pretirano zavezujočega pogoja se odločita C.C. Paige in M.A. Saunders ta posplošeni singularni izrek še dodatno posplošiti. Tako dobimo naslednji izrek:
% !!!!!! Druga posplošitev Singularnega razcepa - IZREK !!!!!
\begin{izrek}[Posplošeni singularni razcep (Paige in Saunders)]
\label{izrek:GSVD} Naj bosta dani matriki $K_A \in \mathbb{R}^{p \times m}$ in $K_B \in \mathbb{R}^{n \times m}$. Potem za $K = \left[\begin{array}{c} K_A \\ K_B \end{array}\right]$ in $k = rang(K)$ obstajajo ortogonalne matrike $U \in \mathbb{R}^{p \times p}$, $V \in \mathbb{R}^{n \times n}$, $W \in \mathbb{R}^{k \times k}$ in $Q \in \mathbb{R}^{m \times m}$, da velja 
\begin{equation} \label{eq4}
U^T K_A Q = \Sigma_A  \left[\begin{array}{cc} W^T R, & 0 \end{array}\right] \hspace{4mm} \text{in} \hspace{4mm} V^T K_B Q = \Sigma_B  \left[\begin{array}{cc} W^T R, & 0 \end{array}\right] \text{,}
\end{equation} kjer sta
$$\Sigma_A = \begin{bmatrix} 
I_A &  & \\
 & D_A & \\
 & & 0_A  
\end{bmatrix} \hspace{4mm} \text{in} \hspace{4mm}
\Sigma_B = \begin{bmatrix} 
0_B &  & \\
 & D_B & \\
 & & I_B  
\end{bmatrix} \text{,}$$ 
$R \in \mathbb{R}^{k \times k}$ je nesingularna matrika, matriki $I_A \in \mathbb{R}^{r \times r}$ in $I_B \in \mathbb{R}^{(k-r-s) \times (k-r-s)}$ identični matriki, kjer je 
$$r = rang(K) - rang(K_B) \hspace{4mm} \text{in} \hspace{4mm} s = rang(K_A) + rang(K_B) - rang(K),$$
$0_A \in \mathbb{R}^{(p-r-s) \times (k-r-s)}$ in $0_B \in \mathbb{R}^{(n-k+r) \times r}$ ničelni matriki, ki imata lahko tudi ničelno število vrstic ali stolpcev, matriki
$D_A = diag(\alpha_{r+1},..., \alpha_{r+s})$ in $D_B = diag(\beta_{r+1},..., \beta_{r+s})$ pa diagonalni matriki, ki zadoščata pogoju
$$1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0 \hspace{4mm} \text{in} \hspace{4mm} 0 < \beta_{r+1} \leq \ldots \leq \beta_{r+s} < 1$$
pri
\begin{equation} \label{alpha+beta-GSVD}
\alpha_i^2 + \beta_i^2 = 1 \hspace{3mm} \text{za} \hspace{3mm} i = r+1,\ldots, r+s
\text{.}
\end{equation}
\end{izrek}
% !!!!! Dokaz druge posplošitve sing. razcepa !!!!!!!!
\begin{proof}
%Definiramo matriko $K \in \mathbb{R}^{(p+n) \times m}$, ki je sestavljena iz matrik $K_A$ in $K_B$ kot $$K:= \begin{bmatrix} 
%K_A \\
%K_B 
%\end{bmatrix}.$$
%Z $k$ označimo rang te matrike $K$, torej $k = rang(K)$.
Izračunamo singularni razcep matrike  $K$. Tako dobimo ortogonalni matriki $P \in \mathbb{R}^{(p+n) \times (p+n)}$ in $Q \in \mathbb{R}^{m \times m}$, da velja 
\begin{equation} P K Q^T = 
\begin{bmatrix} 
R & 0_{k, m-k} \\
0_{n+p-k, k} & 0_{n+p-k, m-k} 
\end{bmatrix} \text{,} \label{eq:5}
\end{equation}
kjer je $R \in \mathbb{R}^{k \times k}$ diagonalna matrika ranga $k$. Matriki $Q$ in $P$ razdelimo na podmatrike
$$  Q = \left[Q_1 \hspace{2mm} Q_2 \right]
\hspace{4mm} \text{in} \hspace{4mm}
P = \left[P_1 \hspace{2mm} P_2 \right]
=
\begin{bmatrix} 
P_{11} & P_{12} \\
P_{21} & P_{22} 
\end{bmatrix},
$$
kjer je matrika $Q_1 \in \mathbb{R}^{m \times k}$ sestavljena iz prvih $k$ stolpcev matrike $Q$, matrika $Q_2 \in \mathbb{R}^{m \times (m-k)}$ pa iz preostalih $m-k$ stolpcev matrike $Q$, podmatrike matrike $P$ pa so sledečih dimenzij: $P_{11} \in \mathbb{R}^{p \times k}$, $P_{12} \in \mathbb{R}^{p \times (p+n-k)}$, $P_{21} \in \mathbb{R}^{n \times k}$ in $P_{22} \in \mathbb{R}^{n \times (p+n-k)}$.
%, matrika $P_1 \in \mathbb{R}^{(p+m) \times k}$ iz prvih $k$ stolpcev matrike $P$ in njena podmatrika $P_{11} \in \mathbb{C}^{p \times t}$ pa iz prvih m vrstic matrike $P_1$.

Ker je $P$ ortogonalna matrika velja $\norm{P}_2 \leq 1$ in posledično tudi $\norm{P_{11}}_2 \leq \norm{P_{1}}_2 \leq \norm{P}_2 \leq 1$. Posledično nobena singularna vrednost matrike $P_{11}$ ni večja od $1$. %Velja po izreku iz numeričnih metod, ne vem kako se kliče

Singularni razcep podobno kot na matriki $K$ naredimo tudi na matriki $P_{11}$. Tako dobimo ortogonalni matriki $U \in \mathbb{R}^{m \times m}$ in $W \in \mathbb{R}^{k \times k}$, da velja $$ U^T P_{11} W = \Sigma_A \text{,}$$ kjer je $$\Sigma_A = 
\begin{bmatrix} 
I_r &  & \\
 & D_A & \\
 & & 0_A  
\end{bmatrix} \text{,}$$ kjer je $r$ geometrična večkratnost lastne vrednosti $1$, matrika $I_r$ identična matrika dimenzije $r \times r$, matrika $D_A =
\begin{bmatrix}
\alpha_{r+1} & & \\
 & \ddots & \\
 & & \alpha_{r+s}
\end{bmatrix}$ diagonalna matrika, kjer $r+s$ predstavlja rang matrike $P_{11}$ in $0_A \in \mathbb{R}^{(p-r-s) \times (k-r-s)}$ je ničelna matrika, ki ima lahko nič vrstic ali nič stolpcev.
% za katere velja $1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0.$


Matriko $P_{21} W$ množimo z ortogonalnimi transformacijami tako, da uničimo vse elemente v zgornjem delu matrike $P_{21} W$. Tako dobimo ortogonalno in simetrično matriko $V$, da velja $V \in \mathbb{R}^{n \times n}$, da velja $$ V^H P_{21} W = L = (\ell_{ij})_{i, j} =
\begin{bmatrix} 
0 & 0 \\
0 & L_1
\end{bmatrix} 
,$$
%Matriko $P_{21} W$ iz leve množimo z matriko Householderjevih zrcaljenj $V \in \mathbb{R}^{n \times n}$, da velja $$ V^H P_{21} W = L = (\ell_{ij})_{i, j} %=
%\begin{bmatrix} 
%0 & 0 \\
%0 & L_1
%\end{bmatrix} 
%,$$
%\textbf{(-- POVEJ KATERIH DIMENZIJ SO TE NIČELNE MATRIKE!)} 
kjer je matrika $L_1$ spodnjetrikotna z diagonalnimi elementi večjimi od $0$. Matrika ortogonalnih transformacij $V$ je tu sestavljena kot produkt matrik Householderjevih zrcaljenj $\tilde{P_1}$, $\tilde{P_2}$, \ldots, $\tilde{P_k}$, ki jih dobimo tako, da začnemo elemente zgornjega dela matrike $P_{21} W$ uničevati iz desne proti levi in za to uporabimo manj stabilno verzijo Householderjevih zrcaljenj, tako, da je zadoščeno pogoju, da so diagonalni elementi matrike $L_1$ večji od $0$.
Velja torej $$V = \tilde{P_1} \tilde{P_2} \dots \tilde{P_k} \text{,}$$
kjer je $\tilde{P_k} = I - \frac{2}{w_k^T w_k} w_k w_k^T$, $w_k = x_k - \norm{x_k}_2 e_n$, kjer $e_n$ predstavlja enotski vektor z enico na $n$-tem mestu, $x_j$ pa predstavlja $j$-ti stolpec matrike $P_{21} W$.


Opazimo lahko, da velja spodnja enakost
$$
\begin{bmatrix} 
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{bmatrix}
\begin{bmatrix}
P_{11} \\
P_{21}
\end{bmatrix} W = 
\begin{bmatrix}
U^T P_{11} W \\
V^TP_{21} W
\end{bmatrix} =
\begin{bmatrix}
\Sigma_A \\
L
\end{bmatrix}.
$$
Zgornja matrika $\begin{bmatrix}
\Sigma_A \\
L
\end{bmatrix}$ je ortogonalna, saj je produkt ortogonalnih matrik. To implicira obliko matrike $ \begin{bmatrix}
\Sigma_A \\
L
\end{bmatrix}$, torej
$$
\begin{bmatrix}
\Sigma_A \\
L
\end{bmatrix} =
\left[
\begin{array}{c;{2pt/2pt}c;{2pt/2pt}c}
I_r & & \\ 
\hdashline[2pt/2pt]
 & D_A & \\ 
\hdashline[2pt/2pt]
 & & 0_{m-r-s, k-r-s} \\ 
\hdashline[2pt/2pt]
0_{p-k+r, r} & & \\ 
\hdashline[2pt/2pt]
 & D_B & \\ 
\hdashline[2pt/2pt]
 & & I_{k-r-s, k-r-s}
\end{array} \right] 
%=
%\begin{bmatrix}
%I_{r, r} &  & \\
% & D_A & \\
% & & 0_{m-r-s, k-r-s} \\
%0_{p-k+r, r} &  & \\
% & D_B & \\
% & & I_{k-r-s, k-r-s}
%\end{bmatrix} 
\text{,}
$$ kjer je matrika $D_B$ diagonalna matrika 
$  D_B = 
\begin{bmatrix}
\beta_{r+1} & & \\
 & \ddots & \\
 & & \beta_{r+s}
\end{bmatrix}$.
Za matriko $L$ pa velja $L = \Sigma_B =
\begin{bmatrix}
0_{p-k+r, r} & & \\ 
 & D_B & \\ 
 & & I_{k-r-s, k-r-s}
\end{bmatrix}$.
Iz ortogonalnosti matrike $\begin{bmatrix}
\Sigma_A \\
L
\end{bmatrix}$ sledi tudi, da so njeni stolpci ortonormirani, iz česar sledi $\alpha_{r+i}^2 + \beta_{r+i}^2 = 1$ za $i = 1, \ldots, s$.

Iz enačbe  \textit{\eqref{eq:5}} dobimo
\begin{equation} \label{eq6}
\begin{split}
\begin{bmatrix} 
K_A \\
K_B 
\end{bmatrix} Q =
\begin{bmatrix} 
P_{11} & P_{12} \\
P_{21} & P_{22} 
\end{bmatrix}
\begin{bmatrix} 
R & 0_{k, m-k} \\
0_{n+p-k, k} & 0_{n+p-k, m-k}  
\end{bmatrix} =
\\
\begin{bmatrix} 
P_{11}R & 0_{m, m-k} \\
P_{21}R & 0_{p, m-k} 
\end{bmatrix} =
\begin{bmatrix} 
U \Sigma_A W^T R & 0_{m, m-k} \\
V \Sigma_B W^T R & 0_{p, m-k} 
\end{bmatrix} \text{,}
\end{split}
\end{equation}
iz česar sledi
$$
P_{11} = U \Sigma_A W^T
\hspace{2mm} \text{in} \hspace{2mm}
P_{21} = V \Sigma_B W^T \text{.}
$$
Če začetno enačbo iz enačbe \textit{\eqref{eq6}} pomnožimo iz leve z matriko 
$
\begin{bmatrix}
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{bmatrix}
$ dobimo 
$$
\begin{bmatrix}
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{bmatrix}
\begin{bmatrix} 
K_A \\
K_B 
\end{bmatrix} Q =
\begin{bmatrix}
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{bmatrix}
\begin{bmatrix} 
U \Sigma_A W^T R & 0_{m, m-k} \\
V \Sigma_B W^T R & 0_{p, m-k} 
\end{bmatrix} \text{,}
$$
iz česar sledi
%\begin{equation} \label{ena7}
$$
U^T K_A Q = 
\Sigma_A  \left[\begin{array}{cc} W^T R, & 0 \end{array}\right] \hspace{4mm} \text{in} \hspace{4mm} V^T K_B Q = \Sigma_B  \left[\begin{array}{cc} W^T R, & 0 \end{array}\right] \text{,}
%\end{equation}
$$
s čimer smo dokazali izrek.
\end{proof}

Iz posplošenega singularnega razcepa, ki sta ga definirala Paige in Saunders neposredno sledi Van Loanova posplošitev singularnega razcepa. 
S preoblikovanjem enačbe \textit{\eqref{eq4}}, dobimo
$$U^T K_A Q = \left[\begin{array}{cc} \Sigma_A, & 0 \end{array}\right]
\begin{bmatrix}
W^TR & 0 \\
0 & I 
\end{bmatrix} \text{.}$$
Inverz matrike
$\begin{bmatrix}
W^TR & 0 \\
0 & I 
\end{bmatrix} $
je kar matrika 
$\begin{bmatrix}
R^{-1} W & 0 \\
0 & I 
\end{bmatrix} \text{,}$
saj veljata obe enakosti iz definicije inverza, torej
$$\begin{bmatrix}
R^{-1} W & 0 \\
0 & I 
\end{bmatrix}
\begin{bmatrix}
W^TR & 0 \\
0 & I 
\end{bmatrix}
= I \hspace{3mm}\text{in} \hspace{3mm}
\begin{bmatrix}
W^TR & 0 \\
0 & I 
\end{bmatrix}
\begin{bmatrix}
R^{-1} W & 0 \\
0 & I 
\end{bmatrix}
= I \text{.}$$
Tako tu očitno sledi
$$ U^T K_A Q
\begin{bmatrix}
R^{-1} W & 0 \\
0 & I 
\end{bmatrix}
=
\left[\begin{array}{cc} \Sigma_A, & 0 \end{array}\right] \text{.}
$$
Matriko $X$ definiramo kot
$$ Q
\begin{bmatrix}
R^{-1} W & 0 \\
0 & I 
\end{bmatrix}
$$ in dobimo ravno Van Loanovo posplošitev razcepa
$$ U^T K_A X = \left[\begin{array}{cc} \Sigma_A, & 0 \end{array}\right] \text{.}
$$
Podobno lahko pokažemo tudi za matriko $K_B$, za katero iz enačbe \textit{\eqref{eq4}} dobimo
$$ V^T K_B X = \left[\begin{array}{cc} \Sigma_B, & 0 \end{array}\right] \text{.}
$$
Tako je definirana matrika $X$ ravno iskana matrika iz prve posplošitve singularnega razcepa in matriki $\left[\begin{array}{cc} \Sigma_A, & 0 \end{array}\right]$ in $\left[\begin{array}{cc} \Sigma_B, & 0 \end{array}\right]$ ravno iskani diagonalni matriki.

Za nadaljnje delo definirajmo matrike
$$H_W :=\left[A_1 - c^{(1)}e^{(1)^T}, \ldots, A_k - c^{(k)}e^{(k)^T}\right] \text{,}$$
$$H_B := \left[(c^{(1)} - c)e^{(1)^T}, \ldots,(c^{(k)} - c) e^{(k)^T}\right]$$
in
$$H_M := \left[a_1 - c, \ldots, a_n - c\right] = A - ce^T = H_W + H_B\text{,}$$
kjer velja
$e^{(i)} = (1,\ldots, 1) ^T \in \mathbb{R}^{ n_i \times 1 }$ in $e =  (1,\ldots, 1) ^T \in \mathbb{R}^{ n \times 1 }$.

S pomočjo teh matrik lahko definiramo tudi matrike razpršenosti podatkov. Matriko $S_W$ lahko definiramo kot produkt matrike $H_W$ z njeno transponiranko, torej
$$S_W = H_W H_W^T \text{,}$$
matriko $S_B$ lahko definiramo na podoben način kot
$$S_B = H_B H_B^T \text{,}$$
prav tako pa tudi matriko $S_M$
$$S_M = H_M H_M^T \text{.}$$

S pomočjo tega razcepa lahko tudi na drugačen način pokažemo, da je matrika $S_W$, kadar velja $m>n$, singularna. Razvidno je namreč, da je ta matrika $S_W$ definirana kot produkt dveh matrik, kjer je matrika $H_W$ dimenzije $m \times n$, matrika $H_W^T$ pa dimenzije $n \times m$. V kolikor velja $m>n$, sta tako ti dve matrike največ ranga $n$.
Ker pa za rang matrike velja, da je rang produkta dveh matrik navzgor omejen z manjšim izmed rangov teh dveh posameznih matrik ($\text{rang}(AB) \leq \min(\text{rang}(A), \text{rang}(B)$), je posledično tudi matrika $S_W$ največ ranga $n$ in torej očitno singularna.

%\textbf{-- Tu mogoče dopiši glede matrik $K_A$ in $K_B$}

\section{Matematična rešitev problema}
V tem odstavku prikažemo uporabo posplošenega singularnega razcepa v namen razširjene uporabe posplošene diskriminantne analize.
 
\subsection{Optimizacija optimizacijskega kriterija $J_1 = \text{sled}({S^\ell_2}^{-1} S^\ell_1)$ za nesingularno matriko $S_2$}
Tu izhajamo iz optimizacije optimizacijskega kriterija 
$$J_1(G) = \text{sled}\left((G^T S_2 G)^{-1} (G^T S_1 G)\right)$$
z izbiro optimalne preslikave $G$, kjer sta matriki $S_1$ in $S_2$ izbrani izmed matrik $S_W$, $S_B$ in $S_M$. Ko je matrika $S_2$ nesingularna, je sestavljena kot produkt matrike in transponiranke te matrike in je zato simetrično pozitivno definitna (posledično so vse lastne vrednosti te matrike večje ali enake 0). Za simetrično pozitivno definitno matriko pa obstaja razcep Choleskega; tako vemo da obstaja spodnjetrikotna matrika $V$ s pozitivnimi elementi na diagonali, da velja
$$ S_2 = V V^T \text{.}
$$
Oglejmo si posplošeni problem lastnih vrednosti za matriki $S_2$ in $S_1$, kjer za ti dve matriki iščemo takšen $\lambda_i \in \mathbb{R}$ in takšen neničelen vektor $x_i \in \mathbb{R}^m$, da velja
\begin{equation} \label{pos-prob-last-v}
S_1 x_i = \lambda_i S_2 x_i
\text{.}
\end{equation}
Če sedaj matriko $S_2$ nadomestimo z matriko $VV^T$, ki jo dobimo iz razcepa Choleskega dobimo
$$
S_1 x_i = \lambda_i V V^T x_i = V \lambda_i V^T x_i
\text{,}
$$
kar lahko iz leve pomnožimo z $V^{-1}$, saj vemo da za matriko $V$ obstaja inverz, in tako dobimo
$$
V^{-1} S_1 x_i = \lambda_i V^T x_i
\text{.}
$$
Enačbo lahko dodatno razčlenimo
$$
V^{-1} S_1 V^{-T} V^T x_i = \lambda_i V^T x_i
\text{.}
$$
Ker je matrika $S_1$ simetrična, je tudi matrika $V^{-1} S_1 V^{-T}$ simetrična, saj velja
$$
(V^{-1} S_1 V^{-T})^T =  (V^{-T})^T S_1^T (V^{-1})^T =  V^{-1} S_1 V^{-T}
\text{.}
$$
Simetrično matriko pa lahko diagonaliziramo v bazi ortonormiranih lastnih vektorjev in tako dobimo takšno matriko $Y$, da velja $Y Y^T = Y^T Y$ in 
$$
V^{-1} S_1 V^{-T} = Y \Lambda Y^T
\text{,}
$$
kjer je $\Lambda = \text{diag}\left(\lambda_1, \ldots, \lambda_m \right)$ diagonalna matrika. S preoblikovanjem zgornje enačbe pa lahko dobimo
$$
S_1 = VY \Lambda Y^TV^T = X \Lambda X^T
\text{,}
$$
kjer smo dodatno definirali $X := VY$.
Poleg matrike $S_1$ pa lahko preoblikujemo tudi matriko $S_2$
$$
S_2 = VV^T = VYY^TV^T = XX^T
\text{.}
$$
Ker ima matrika $X$ inverz, lahko zgornji enačbi tudi obrnemo, tako da je $X^T S_1 X = \Lambda$ in $X^T S_2 X = I_m$.

Iz posplošenega problema lastnih vrednosti $\textit{\eqref{pos-prob-last-v}}$ lahko vidimo, da sta $\lambda_i$ in $x_i$ ravno lastna vrednosti in lastni vektor za dano lastno vrednost $\lambda_i$ za matriko $S_2^{-1} S_1$. Ker je matrika $S_1$ simetrična pozitivno definitna, vemo, da so vse njene lastne vrednosti $\lambda_i \geq 0$ za $i = 1, \ldots, m$. Z uporabo permutacijskih matrik, lahko matriko $\Lambda$ preuredimo tako, da za  $q = \text{rang} \left( S_1 \right)$ velja $\lambda_1 \geq \ldots \geq \lambda_q > \lambda_{q+1} = \ldots = \lambda_m = 0$.

Optimizacijski kriterij ima tako sledečo obliko
\begin{equation} \label{pospJ1}
\begin{split}
J_1\left(G\right)
=
\text{sled} \left( (G^T S_2 G)^{-1} G^T S_1 G \right)
\\
=
\text{sled} \left( (G^T X^{-T}X^{-1} G)^{-1} G^T X^{-T} \Lambda X^{-1} G \right)
\\
= 
\text{sled} \left( (\tilde{G}^T \tilde{G})^{-1} \tilde{G}^T \Lambda \tilde{G} \right)
\text{,}
\end{split}
\end{equation}
kjer je matrika $\tilde{G} := X^{-1} G \in \mathbb{R}^{m \times \ell}$. Ker vemo, da ima matrika $X$ poln rang, ima matrika $\tilde{G}$ rang enak številu stolpcev, torej $\ell$, in tako lahko na njej naredimo $QR$ razcep in tako dobimo matriko $Q \in \mathbb{R}^{m \times \ell}$, ki ima ortonormirane stolpce in nesingularno matriko $R \in \mathbb{R}^{\ell \times \ell}$, da velja $\tilde{G} = QR$. Tako lahko zgornjo enačbo $\textit{\eqref{pospJ1}}$ dodatno preoblikujemo
\begin{align*}
J_1(G)
=
\text{sled} \left( ((QR)^T QR)^{-1} (QR)^T \Lambda QR \right)
\\
=
\text{sled} \left( (R^T Q^T QR)^{-1} R^T Q^T \Lambda QR \right)
\\
=
\text{sled} \left( (R^T R)^{-1} R^T Q^T \Lambda QR \right)
\\
=
\text{sled} \left( R^{-1} Q^T \Lambda QR \right)
\text{,}
\end{align*}
ker pa vemo, da imata podobni matriki enako sled, lahko to matriko množimo z $R$ iz leve in z $R^{-1}$ iz desne. Tako dobimo
\begin{align*}
J_1(G)
=
\text{sled} \left(Q^T \Lambda QR  (R)^{-1} \right)
\\
=
\text{sled} \left(Q^T \Lambda Q \right)
\text{.}
\end{align*}
Tako lahko vidimo, da ko smo matriki $S_1$ in $S_2$ enkrat diagonalizirali, je maksimizacija optimizacijskega kriterija odvisna le še od matrike $Q$, ki predstavlja ortonormirano bazo za matriko $X^{-1} G$, torej
\begin{align*}
\max_G J_1(G) = \max_{Q^TQ=I} \text{sled} \left(Q^T \Lambda Q \right)
\text{.}
\end{align*}
Ker pa vemo, da je sled matrike enaka vsoti lastnih vrednosti in da ima matrika $Q$ ortonormirane stolpce, velja
\begin{align*}
\max_{Q^TQ=I} \text{sled} \left(Q^T \Lambda Q \right)
\\
\leq
\lambda_1 + \cdots + \lambda_q
\\
\text{sled} \left( S_2^{-1} S_1 \right)
\text{.}
\end{align*}
Za vsak $\ell \geq q$ optimizacijski kriterij doseže svoj maksimum za 
$$ Q = 
\begin{bmatrix}
I_{\ell} \\
0_{m-\ell, \ell}
\end{bmatrix}
\hspace{4mm}
\text{oziroma za}
\hspace{4mm}
G = 
X 
\begin{bmatrix}
I_{\ell} \\
0_{m-\ell, \ell}
\end{bmatrix}
R
\text{.}
$$
Preslikava $G$, za katero maksimizacijski kriterij doseže svoj maksimum, pa ni edinstvena, saj za katerokoli nesingularno matriko $W \in \mathbb{R}^{\ell \times \ell}$ velja
\begin{align*}
J_1(GW) = 
\text{sled}\left( (W^T G^T S_2 G W)^{-1} W^T G^T S_1 GW \right)
\\
=
\text{sled}\left( W^{-1}(G^T S_2 G)^{-1}W^{-T} W^T G^T S_1 GW \right)
\\
=
\text{sled}\left( W^{-1}(G^T S_2 G)^{-1}G^T S_1 GW \right)
\text{,}
\end{align*}
ker pa velja, da imata podobni matriki enako sled, lahko zgornjo enačbo množimo iz leve z $W$ in iz desne z $W^{-1}$ in tako dobimo
\begin{align*}
\text{sled}\left((G^T S_2 G)^{-1} G^T S_1 GW W^{-1} \right)
=
J_1(G)
\text{.}
\end{align*}
V kolikor za nesingularno matriko $W$ izberemo $R^{-1}$ (ta obstaja, saj je $R$ po definiciji nesingularna), je maksimum optimizacijskega kriterija $J_1(G)$ dosežen tudi za 
$$ G = X 
\begin{bmatrix}
I_{\ell} \\
0_{m-\ell, \ell}
\end{bmatrix}
\text{.}
$$
Tako smo ugotovili, da za $\ell \geq q = rang(S_1)$ velja
$$
\text{sled} \left( (G^T S_2 G)^{-1} G^T S_1 G \right)
=
\text{sled} \left( S_2^{-1} S_1 \right)
\text{,}
$$
v kolikor preslikavo $G \in \mathbb{R}^{m \times \ell}$
sestavimo iz $\ell$ lastnih vektorjev matrike $S_2^{-1} S_1$, ki pripadajo $\ell$ največjim lastnim vrednostim te matrike.

Po ugotovitvi iz prvega poglavja, je matrika $S_2$ lahko singularna le, ko velja $m \leq n$, oziroma, ko število pridobljenih podatkov večje od dimenzije posameznega podatka. V nasprotnem primeru trenutne rešitve za maksimizacijski kriterij ne moremo uporabiti. Za nadaljevanje zapišimo $\lambda_i$ iz enačbe $\textit{\eqref{pos-prob-last-v}}$ kot $\alpha_i^2/\beta_i^2$ in tako se naš problem posploši na
\begin{equation} \label{alpha-beta}
\beta_i^2 S_1 x_i = \alpha_i^2 S_2 x_i
\text{.}
\end{equation}
V naslednjem poglavju maksimizacijo optimizacijskega kriterija $J_1(G)$ posplošimo tudi na primer, ko je matrika $S_2$ singularna.

\subsection{Posplošitev optimizacijskega kriterija $J_1(G) = \text{sled}((S^\ell_W)^{-1}S^\ell_B)$ za singularno matriko $S_2$}

V nadaljevanju vzemimo maksimizacijo prejšnjega optimizacijskega kriterija $J_1$, kjer matrik $S_1$ in $S_2$ ne izbiramo več, temveč te določimo: $S_1 = S_B$ in $S_2 = S_W$. Želimo poiskati preslikavo $G$, ki zadošča
\begin{equation} \label{pogoji-G}
\max_G \text{sled} \left( G^T S_W G \right)
\hspace{4mm}
\text{in}
\hspace{4mm}
\min_G \text{sled} \left( G^T S_B G \right)
\text{.}
\end{equation}
Za iskanje $x_i$-jev iz enačbe \textit{\eqref{alpha-beta}} uporabimo posplošeni singularni razcep. V ta namen naredimo drugo posplošitev singularnega razcepa na matriki 
$K := 
\begin{bmatrix}
H_B^T \\
H_W^T
\end{bmatrix}
\in \mathbb{R}^{2n \times m}$.
Tako dobimo takšne ortogonalne matrike $U \in \mathbb{R}^{n \times n}$, $V \in \mathbb{R}^{n \times n}$ in $X \in \mathbb{R}^{m \times m}$ ter takšni matriki $\Sigma_A$ in $\Sigma_B$, da velja
$$
H_B^T = U \left(\Sigma_A, \hspace{2mm} 0 \right) X^{-1}
\hspace{4mm}
\text{in}
\hspace{4mm}
H_W^T = V \left(\Sigma_B, \hspace{2mm} 0 \right) X^{-1}
\text{,}
$$
kjer sta matriki $\Sigma_A$ in $\Sigma_B$ sledečih oblik
$
\Sigma_A = 
\begin{bmatrix} 
I_r &  & \\
 & D_A & \\
 & & 0_{n-r-s, k-r-s}  
\end{bmatrix}
$ in
$
\Sigma_B =
\begin{bmatrix}
0_{n-k+r, r} & & \\ 
 & D_B & \\ 
 & & I_{k-r-s}
\end{bmatrix}
$, matriki $D_A$ in $D_B$ sta pa naslednji
$
D_A =
\begin{bmatrix}
\alpha_{r+1} & & \\
 & \ddots & \\
 & & \alpha_{r+s}
\end{bmatrix}
$ in 
$
D_B = 
\begin{bmatrix}
\beta_{r+1} & & \\
 & \ddots & \\
 & & \beta_{r+s}
\end{bmatrix}
$, kjer je
$r = \text{rang}(K) - \text{rang}(K_B)$,  $s = \text{rang}(K_A) + \text{rang}(K_B) - \text{rang}(K)$ ter $k = \text{rang}(K)$. Za elemente matrik $D_A$ in $D_B$ pa velja
$1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0$ in $ 0 < \beta_{r+1} \leq \ldots \leq \beta_{r+s} < 1$.
Sedaj si podrobneje oglejmo matriki $H_B H_B^T$ in $H_W H_W^T$. Od prej vemo, da je $S_B = H_B H_B^T$ in $S_W = H_W H_W^T$.
Za produkt matrik $H_B H_B^T$ po zgornjem razcepu velja
\begin{align*}
H_B H_B^T =
X^{-T}
\begin{bmatrix}
\Sigma_A^T \\
0^T
\end{bmatrix}
U^T
U \left[\Sigma_A, \hspace{2mm} 0 \right] X^{-1}
\\
= X^{-T}
\begin{bmatrix}
\Sigma_A^T \Sigma_A & 0 \\
0 & 0
\end{bmatrix}
X^{-1} \text{,}
\end{align*}
kjer za matrika $\Sigma_A^T \Sigma_A \in \mathbb{R}^{k \times k}$ enaka diagonalni matriki $\text{diag}\left(\alpha_1^2, \ldots, \alpha_k^2 \right)$. V kolikor tu gledamo le $i$-ti stolpec ortogonalne matrike $X$, ki ga označimo z $x_i$, ter dodatno definiramo $\alpha_i = 0$ za $i = k+1, \ldots, m$, za $i = 1, 2, \ldots, m$ dobimo
$$
x_i^T H_B H_B^T x_i = \alpha_i^2
\text{,}
$$
če pa to enačbo množimo z leve z $x_i$ dobimo
\begin{equation} \label{H_B}
H_B H_B^T x_i = \alpha_i^2 x_i
\text{.}
\end{equation}
Za produkt matrik $H_W H_W^T$ po podobnem izračunu kot za $H_B H_B^T$ velja
\begin{align*}
H_W H_W^T =
X^{-T}
\begin{bmatrix}
\Sigma_B^T \Sigma_B & 0 \\
0 & 0
\end{bmatrix}
X^{-1} \text{.}
\end{align*}
Prav tako velja tudi 
\begin{equation} \label{H_W}
H_W H_W^T x_i = \beta_i^2 x_i
\text{,}
\end{equation}
kjer smo dodatno definirali $\beta_i = 0$ za $i = k+1, \ldots, m$.

%\textbf{-- DOPOLNI!}

Če združimo enačbi \textit{\eqref{H_W}} in \textit{\eqref{H_B}}, dobimo enačbo
$$
\frac{1}{\beta_i^2} H_W H_W^T x_i = \frac{1}{\alpha_i^2} H_B H_B^T x_i
\text{,}
$$
kar se preoblikje v
\begin{equation} \label{alpha-beta-1}
\alpha_i^2 H_W H_W^T x_i = \beta_i^2 H_B H_B^T x_i
\text{.}
\end{equation}
Tako dobim enak problem kot v \textit{\eqref{alpha-beta}}.
Najprej bomo potegnili vzporednice med to metodo in metodo iz prejšnjega poglavja, ki velja le za nesingularno matriko $S_W$, nato pa si bomo pogledali primer rešitve tega problema za singularno matriko $H_W$.

\subsubsection{Matrika $H_W$ polnega ranga}
V tem primeru bo zagotovo veljalo $n > m$. Matika $H_W$ bo v tem primeru imela poln stolpični rang, torej $\text{rang}(H_W) = m$, iz česar pa sledi $r = m - m = 0$, $s = \text{rang}(H_B^T) + m - m = \text{rang}(H_B^T)$ ter $k = m$. Od tod sledi $\beta_i \neq 0 \hspace{2mm}  \text{za} \hspace{2mm}\forall \hspace{1mm} i = 1, \ldots, m \text{.}$ Posledično lahko v enačbi \textit{\eqref{alpha-beta-1}} delimo z $\beta_i$ in dobimo 
$$
\frac{\alpha_i^2}{\beta_i^2} H_W H_W^T x_i =  H_B H_B^T x_i
\text{.}
$$
Ker pa nam posplošeni singularni razcep vrne pare singularnih vrednosti $(\alpha_i, \beta_i)$ urejene v sledečem vrstnem redu $1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0$ in $0 < \beta_{r+1} \leq \ldots \leq \beta_{r+s} < 1$, velja, da so koeficienti $\lambda_i = \frac{\alpha_i^2}{\beta_i^2}$ v nenaraščajočem vrstnem redu, saj za poljuben $i \in {1, \ldots, s}$ velja
$$
\frac{\alpha_{r+i}}{\beta_{r+i}} = 
\frac{\alpha_{r+i}}{\beta_{r+i+1}} \frac{\beta_{r+i + 1}}{\beta_{r+i}} 
\geq 
\frac{\alpha_{r+i}}{\beta_{r+i+1}}
\geq
\frac{\alpha_{r+i+1}}{\beta_{r+i+1}}
\text{,}
$$
%so koeficienti $\frac{\alpha_i}{\beta_i}$ nenaraščajoči, 
dodatno pa iz $\alpha_i \geq 0$ in $\beta_i \geq 0$ za $i = 1, \ldots, m$ sledi, da so tudi koeficienti $\frac{\alpha_i^2}{\beta_i^2}$ v nenaraščajočem vrstnem redu. Sledi, da za optimalno preslikavo $G$ potrebujemo le prvih $k - 1$ stolpcev matrike $X$, saj v tem primeru velja kar $k = m$, mi pa želimo preslikavo, ki slika v prostor dimenzije manjše od $m$.

%\textbf{-- Zakaj ravno k-1?}

\subsubsection{Matrika $H_W$ nepolnega ranga}
Do tega primera, ko je matrika $H_W$ nepolnega ranga, pride vedno, ko velja $m>n$. Tu tako ne moremo definirati lastnih vektorjev matrike $S_W^{-1} S_B$ in tako klasična diskriminantna analiza propade. Zamislimo si, da imamo posplošeni singularni vektor $x_i$, ki leži v jedru preslikave $S_W$ (torej $x_i \in \ker (S_W)$).  Iz \textit{\eqref{alpha-beta}} vidimo, da potem velja ena izmed možnosti: ta vektor leži tudi v jedru preslikave $S_B$ ali pa je pripadajoča singularna vrednost $\beta_i$ enaka 0.

Tako ločimo obe možnosti:
\begin{enumerate}
\item{$x_i \in \ker (S_W) \cap \ker (S_B)$}

V tem primeru je enačbi \textit{\eqref{alpha-beta}} zadoščeno za poljuba $\alpha_i$ in $\beta_i$. To bo primer za $m-k$ desnih stolpcev preslikave $X$ (ti stolpci so očitno v jedru preslikave $S_W$, saj se po \textit{\eqref{eq:5}} preslikajo v 0). Premislimo, ali se nam te stolpce preslikave $X$ splača vključiti v preslikavo $G$. Velja
$$
\text{sled}(G^T S_B G) = \sum_j g_j^T S_B g_j
\hspace{2mm}
\text{in}
\hspace{2mm}
\text{sled}(G^T S_W G) = \sum_j g_j^T S_W g_j
\text{,}
$$
kjer $g_j$ predstavlja $j$-ti stolpec preslikave $G$. Ker velja $x_i^T S_W x_i = 0$ in $x_i^T S_B x_i = 0$, dodajanje teh stolpcev v preslikavo $G$ ne bo imelo vpliva niti na maksimizacijo sledi $\left( G^T S_W G \right)$, niti na minimizacijo sledi $\left( G^T S_B G \right)$ iz \textit{\eqref{pogoji-G}}. Posledično teh stolpcev $x_i$ ne vključimo v $G$.

\item{$x_i \in \ker (S_W) - \ker (S_B) 	\Rightarrow \beta_i = 0$}

Iz $\beta_i = 0$ in \textit{\eqref{alpha+beta-GSVD}} sledi, da je $\alpha_i = 1$, iz česar sledi $\lambda_i = \infty$. Ti $x_i$-ji bodo predstavljali najbolj leve stolpce matike $X$. Če te stolpce vključimo v preslikavo $G$, bomo na pogoje iz \textit{\eqref{pogoji-G}} vplivali tako, da bomo $\text{sled}(G^T S_B G)$ povečali, medtem, ko bo $\text{sled}(G^T S_W G)$ ostala nespremenjena. Tako lahko skledemo, da te stolpce vključimo v $G$.
\end{enumerate}
Tako lahko zaključimo, da tudi ko je matrika $S_W$ singularna, preslikavo $G \in \mathbb{R}^{m \times \ell}$ sestavimo iz $\ell$ levih stolpcev matrike $X$. Od tod sledi naslednji algoritem.


\section{Algoritem}

\section{Zaključek}

\section{Priloge}

\begin{izrek}[Singularni razcep]
\label{izrek:SVD} Za vsako matriko $A \in \mathbb{R}^{m \times n}$, z lastnostjo $m \geq n$, obstaja singularni razcep 
$$A = U \Sigma V^T \text{,}$$
kjer sta $U \in \mathbb{R}^{m \times m}$ in $V \in \mathbb{R}^{n \times n}$ ortogonalni matriki, $\Sigma \in \mathbb{R}^{m \times n}$ je oblike
$$
\Sigma = 
\begin{bmatrix} 
\sigma_1 &  & \\
 & \ddots & \\
 & & \sigma_n  \\
 & & 
\end{bmatrix}$$
in $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_n \geq 0$ so singularne vrednosti matrike $A$.
\end{izrek}
\begin{proof}
Ker je $A^TA$ simetrična pozitivno semidefinitna matrika, so vse njene lastne vrednosti nenegativne. Označimo in uredimo jih kot
$$\sigma_1^2 \geq \sigma_2^2 \geq \ldots \geq \sigma_n^2 \geq 0 \text{.}$$
Ustrezni ortonormirani lastni vektorji $v_1, \ldots, v_n$ zadoščajo
$A^T A v_i = \sigma_i^2 v_i$ za $i = 1, \ldots, n \text{.}$
Naj bo $\sigma_r > 0$ in $\sigma_{r+1} = \cdots = \sigma_n = 0 \text{.}$
Matriko $V$ razdelimo na $V_1 = 
\left[ v_1, \ldots, v_r
\right]$ in $V_2 = 
\left[ v_{r+1}, \ldots, v_n
\right] \text{.}$ Iz
$$ (AV_2)^T (AV_2) = V_2^T A^T A V_2 = V_2^T \left[ 0, \ldots, 0 \right] = 0
$$
sledi $AV_2 = 0 \text{.}$ Sedaj definiramo $u_i := \frac{1}{\sigma_i} Av_i$ za $i = 1, \ldots, r \text{.}$
Vekorji $u_1, \ldots, u_r$ so ortonormirani, saj je
$$ u_i^T u_j = \frac{1}{\sigma_i \sigma_j} v_i^T A^T A v_j = \frac{\sigma_j}{\sigma_i} v_i^T v_j = \delta_{ij} \text{,} \hspace{3mm} i,j = 1, \ldots, r \text{,}
$$
kjer smo v zapisu uproabili t.i. \emph{Kroneckerjev delta}, definiran z $\delta_{ij} = 1$ za $i = j$ in $\delta_{ij} = 0$ za $i \neq j$. Označimo 
$U_1 = 
\left[ u_1 \hspace{2mm} \cdots \hspace{2mm} u_r
\right]$ in dopolnimo z $U_2 = 
\left[ u_{r+1} \hspace{2mm} \cdots \hspace{2mm} u_n
\right]$, da je $ U = \left[ U_1 \hspace{2mm} U_2 \right]$ ortogonalna matrika. Matrika $U^T A V$ ima obliko 
$$
U^T A V = 
\begin{bmatrix} 
U_1^T A V_1 & U_1^T A V_2 \\
U_2^T A V_1 & U_2^T A V_2
\end{bmatrix}
\text{.}
$$
Desna bloka sta zaradi $AV_2 = 0$ enaka $0$. Za $i = 1, \ldots, r$ in $k = 1, \ldots, m$ velja
$$ u_k^T A v_i = \sigma_i u_k^T u_i = \sigma_i \delta{ik} \text{,}
$$
torej $U_2^T A V_1 = 0$ in $U_1^T A V_1 = diag(\sigma_1, \ldots, \sigma_r) \text{.}$ Dobimo singularni razcep $A = U \Sigma V^T \text{,}$ kjer je $S = diag(\sigma_1, \ldots, \sigma_r)$ in 
$$ \Sigma = 
\begin{bmatrix} 
S & 0 \\
0 & 0
\end{bmatrix}
\text{.}$$

\end{proof}
V primeru, ko velja $n>m$, dobimo singularni razcep za $A \in \mathbb{R}^{m \times n}$ tako, da transponiramo singularni razcep $A^T$.
\section{Viri}

\end{document}