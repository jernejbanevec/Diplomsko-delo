% POZOR! - Pazi, da imaš nastavljeno vrednost "pdfLaTeX" v zgornjem okencu
\documentclass[mat1]{article}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{izrek}{Izrek}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}

%za x piši \times
% \in \mathbb{R}^{m \times n}

% Kjer so nejasnosti, dodaj znak oz. opozorilo za dopolnitev ali nejasnost
% \textbf{-- NEJASNOST!}
% \textbf{-- DOPOLNI!}

\begin{document}

\section{Uvod}

\subsection{Abstraktni uvod}
Diskriminantna analiza se že dolga leta uporablja za določevanje lastnosti, ki ohranjajo razlike med razredi. Definirana je kot optimizacijski problem, ki vključuje kovariančne matrike, ki predstavljajo razprešenost podatkov znotraj posameznega razreda in razpršenost oziroma ločenost posameznih razredov. Diskriminantna analiza pa sama po sebi zahteva, da je ena od teh kovariančnih matrik nesingularna, kar omejuje njeno uporabo na matrikah določenih dimenzij. V nadaljevanju tako preučimo več različnih optimizacijskih kriterijev in poskušamo njihovo uporabo razširiti na vse matrike z uporabo posplošenega singularnega razcepa. Na ta način se izognemo pogoju nesingularnosti, ki ga zahteva diskriminantna analiza. Na ta način dobimo posplošeno diskriminantno analizo, ki jo lahko uporabimo tudi kadar je ena matrika nesingularna (v nadaljevanju lahko vidimo, da je matrika nesingularna kadar je velikost vzorca manjša kot pa dimenzija posamezne meritve  \textbf{-- NEJASNOST!}). V nadaljevanju bom testiral učinkovitost posplošene diskriminantne analize in jo, kjer bo to mogoče, primerjal tudi z diskriminantno analizo.

\subsection{Matematični uvod}
Cilj diskriminantne analize je združevati lastnosti originalnih podatkov na način, ki kar najučinkoviteje ločuje med razredi, v katerih so podatki. Pri takšnem združevanju lastnosti podatkov se dimenzija teh podatkov zmanjša na način, ki najbolje ohranja strukture določenih razredov.  

Tu predpostavimo, da so podatki zloženi v matiko $A \in \mathbb{R}^{m \times n}$, kjer $m$ predstavlja dimenzijo posamezne meritve, $n$ pa predstavlja število meritev oz. podatkov. Denimo, da so podatki v matriki $A$ iz $k$ različnih razredov. Tako so stolpci $a_1, a_2, \ldots, a_n$ matrike $A$ združeni v $k$ podmatrik, ki predstavljajo razrede, v katerih so podatki:
$$ A = 
\begin{pmatrix}
A_1, & A_2, & \ldots, & A_k
\end{pmatrix} \text{,}
\hspace{2mm} \text{kjer} \hspace{2mm} A_i \in \mathbb{R}^{m \times n_i} \text{.}
$$ 

Cilj diskriminantne analize najti preslikavo $G^T$, ki v novem, manjdimenzionalnem prostoru, kar najbolje ohranja razrede, v katerih so podatki.
Za preslikavo $G^T$ torej velja: $$G^T : \mathbb{R}^m \rightarrow \mathbb{R}^\ell ,$$ kjer je $\ell \leq m - 1$. Torej preslikava $G^T$ nek $m$-dimenzionalen vektor preslika v nov vektor v $\ell$-dimezionalnem prostoru (navadno velja $\ell \leq m$), v katerem so razredi podatkov ohranjeni, razpršenost podatkov znotraj razredov je zmanjšana, razlike med razredi pa so povečane.

Tu število $n_i$ predstavlja moč indeksne množice razreda $i$. To indeksno množico razreda $i$ označujemo z $N_i$. Očitno velja tudi: $$
\sum_{i=1}^{k}n_i = n \text{.}$$

Za nadaljnje izračune moramo definirati tudi centroid $i$-tega razreda, ki je izračunan kot povprečje stolpcev v $i$-tem razredu, torej: 
$$c^{(i)} = \frac{1}{n_i} \sum_{j \in N_i} a_j
$$
in centroid celotnih podatkov, ki je izračunan kot povprečje vseh stolpcev:
$$c = \frac{1}{n} \sum_{j = 1}^{n} a_j \text{.}
$$

Razpršenost podatkov v razredih, razpršenost vseh podatkov ter razpršenost oziroma razlike med razredi je smiselno predstaviti s pomočjo matrik. Zato v nadaljevanju definiramo matriko, ki predstavlja matriko razpršenosti podatkov znotraj razredov:
$$S_W = \sum_{i = 1}^{k} \sum_{j \in N_i}(a_j - c^{(i)})(a_j - c^{(i)})^T\text{,}$$
matriko, ki predstavlja matriko razpršenosti oz razlik med razred:
$$S_B = \sum_{i = 1}^{k} \sum_{j \in N_i} ( c^{(i)} - c)( c^{(i)} - c)^T = \sum_{i = 1}^{k} n_i ( c^{(i)} - c)( c^{(i)} - c)^T$$
in matriko celotne razpršenosti podatkov:
$$S_M = \sum_{j = 1}^{n} (a_j - c)(a_j - c)^T$$

Med zgoraj definiranimi matrikami velja tudi enakost:
$S_M = S_W + S_B \text{.}$ \textbf{-- DOKAŽI}

S pomočjo preslikave $G^T$ preslikamo v $\ell$-dimezionalen prostor tudi matrike $S_W$, $S_B$ in $S_M$:
$$ S_{W}^{\ell} = G^T S_W G \text{,} \hspace{2mm} S_{B}^{\ell} = G^T S_B G \text{,} \hspace{2mm} S_{M}^{\ell} = G^T S_M G \text{.}
$$

Iz danih matrik razpršenosti podatkov bi radi tvorili kriterij kvalitete razredov. Kriterij kvalitete razredov bi imel visoko vrednost, kadar bi bili razredi, v katerih so podatki, strnjeni in dobro ločeni med seboj. Opazimo lahko, da $sled(S_W)$ predstavlja kako skupaj so si podatki v posameznem razredu, saj velja:
\begin{gather*} 
sled(S_W) = \sum_{t=1}^{m} (\sum_{i = 1}^{k} \sum_{j \in N_i}(a_{j_t} - c_t^{(i)})^2)
= \sum_{i = 1}^{k} \sum_{j \in N_i} ( \sum_{t=1}^{m} (a_{j_t} - c_t^{(i)})^2) \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ a_{j_t} - c_t^{(i)}}_2^2 \text{.}
\end{gather*}

Podobno $sled(S_B)$ predstavlja ločenost med razredi, saj velja:
\begin{gather*} 
sled(S_B) = \sum_{t=1}^{m} (\sum_{i = 1}^{k} \sum_{j \in N_i}(c_t^{(i)} - c_t)^2)
= \sum_{i = 1}^{k} \sum_{j \in N_i} ( \sum_{t=1}^{m} (c_t^{(i)} - c_t)^2) \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ c_t^{(i)} - c_t}_2^2 \text{.}
\end{gather*}

Optimalna preslikava $G^T$ tako maksimizira $sled(S_{B}^{\ell})$ in minimizira $sled(S_{W}^{\ell})$. Smiselen kriterij se tako zdi $$sled( G^T S_B G) / sled( G^T S_W G) \text{,}$$ ki pa ga zaradi težke izračunljivosti aproksimiramo kar z $$sled((S_W^\ell)^{-1}S_B^\ell) \text{.}$$
\textbf{-- NEJASNOST!}

Kljub temu, da je ta optimizacijski kriterij lažje izračunljiv ima svoje pomanjkljivosti. Opazimo lahko, da kriterij lahko uporabimo le v primeru, ko je matrika $S_W^\ell$ nesingularna oz. da kriterija nemorem uporabiti, ko je matrika $S_W^\ell$ singularna (torej kadar je njena determinanta enaka 0). Ker pa za determinanto matrike velja:
$$ det(S_W^\ell) = det(G^T S_W G) = det(G^T) \cdot det(S_W) \cdot det(G) \text{,}
$$
je $det(S_W^\ell)$ enaka 0 kadar je $det(S_W)$ enaka 0, torej kadar je matrika $S_W$ singularna. Do te situacije pa lahko pride kar precej pogosto. Matrika $S_W \in  \mathbb{R}^{m \times m}$ je singularna namreč v vseh primerih, ko je za matriko $A \in  \mathbb{R}^{m \times n}$ velja $m > n$, saj je potem $m$-dimenzionalna matrika $S_W$, ki je sestavljena iz $n$ vektorjev (iz $a_j - c^{(i)} \hspace{2mm} \text{za} \hspace{2mm} \forall i \in \{1,\ldots \text{,} k\} \hspace{2mm} \text{in} \hspace{2mm} \forall j \in N_j $). Iz $n$ vektorjev pa lahko sestavimo le $n$ dimenzionalen prostor, torej bo $m \times m$ matrika iz teh vektorjev očitno singularna, torej bo njena determinanta enaka 0.
Na primer, do tega problema pride v primeru, ko je pridobivanje podatko drago oz. zahtevno in so pridobljeni podatki visokih dimenzij (dimenzija posameznega podatka je večja od števila vseh pridobljenih podatkov).

Poznamo več načinov, kako aplicirati diskriminantno analizo na matriki $A \in \mathbb{R}^{m \times n}$ z $m > n$. V grobem jih ločimo na tiste, kjer dimenzijo zmanjšamo v dveh korakih in na tiste, kjer dimenzijo podatkov zmanjšamo v enem koraku. V prvem načinu se faza diskriminante analize nadaljuje v fazo, v kateri zanemarimo oblike posameznih razredov. Najpopularnejša metoda za prvi del tega procesa je zmanjšanje rang s pomočjo singularnega razcepa. To je tudi glavno orodje metode imenovane principalna komponentna analiza. Kakorkoli, celotna predstava dvostopenjskih načinov je precej občutljiva na zmanjšanje dimenzije v prvi fazi. Sam se bom bolj osredotočil na način, ki posploši diskriminantno analizo tako, da teoretično optimalno zmanjša dimenzijo, brez na bi uvedel dodaten korak. Zato obravnavamo kriterij 
$$ sled((S_2^Y)^{-1} S_1^Y) \hspace{1mm} \text{,}
$$
kjer sta matriki $S_2$ in $S_1$ izbrani iz matrik $S_W$, $S_B$ in $S_M$. Klasična diskriminantna analiza predstavi svojo rešitev s pomočjo posplošenega problema lastnih vrednosti, kadar je matrika $S_2$ nesingularna. Z prestrukturiranjem problema tako, da uporabimo posplošeni singularni razcep, pa razširimo uporabnost diskriminantne analize tudi na primer, ko je marika $S_2$ singularna.

\section{Matematična priprava - posplošeni singularni razcep}
Originalna definicija posplošenega singularnega razcepa (Van Loan)
\begin{izrek}[Posplošeni singularni izrek (Van Loan):]
\label{izrek:SVD} Za matriki $K_A \in \mathbb{R}^{p \times m}$ z $p \geq m$ in $K_B \in \mathbb{R}^{n \times m}$ obstajata ortogonalni matriki $U \in \mathbb{R}^{p \times p}$ in $V \in \mathbb{R}^{n \times n}$ ter nesingularna matrika $X \in \mathbb{R}^{m \times m}$, da velja $$ U^T K_A X = diag(\alpha_1,..., \alpha_m) \text{in} V^T K_B X = diag(\beta_1,..., \beta_q) $$ kjer $q = min(n,m)$, $\alpha_i \geq 0$ za $1 \leq i \leq m$ in  $\beta_i \geq 0$ za  $1 \leq i \leq q$.
\end{izrek}
\begin{proof}
% PREVERI SPODNJE PRETVORBE
% m_a = p
% m_b = n
% k = n
% k_1 = k
% n = m

Iz matrik $K_A$ in $K_B$ tvorimo zdrženo matriko $K = \left(\begin{array}{c} K_A \\ K_B \end{array}\right)$, na kateri naredimo singularni razcep. Iz singularnega razcepa dobimo matriki $Q \in \mathbb{R}^{(p+n) \times (p+n)}$ in matriko $Z_1 \in \mathbb{R}^{m \times m}$, tako da velja 
\begin{equation}
Q^T \left(\begin{array}{c} K_A \\ K_B \end{array}\right) Z_1 = diag(\gamma_1,..., \gamma_m) \hspace{2mm} \text{kjer za} \hspace{1mm}  \hspace{1mm} \text{velja} 
\gamma_1 \geq \ldots \geq \gamma_k > \gamma_{k+1} = \ldots \gamma_m \text{.} \label{eq:1}
\end{equation}
\textbf{-- DOPOLNI! Ali je samo diag za napisat ali celo razširjeno matriko?}
V kolikor matriko $Z_1$ razdelimo na dve matriki: $Z_{11} \in \mathbb{R}^{m \times k}$, ki je sestavljena iz prvih $k$ stolpcev matrike $Z_1$ in $Z_{12} \in \mathbb{R}^{(m-k) \times n}$, ki je sestavljena iz preostalih $m-k$ stolpcev matrike $Z_1$, lahko vidimo da velja:
$$ Q^T K \left(\begin{array}{c} Z_{11} | Z_{12} \end{array}\right) =  \left(\begin{array}{c} Z_{11} | Z_{12} \end{array}\right) \text{.}
$$
Po predpostavki velja $p \geq m$ in ker je očitno tudi $m \geq k$ sledi: $p \geq n \geq k$. Sedaj definirajmo matriko 
$$ D := diag(\gamma_1,..., \gamma_k \in \mathbb{R}^{k \times k}) \text{.}
$$
Tako iz zgornje enačbe \textit{\eqref{eq:1}} dobimo:
\begin{equation}
\begin{pmatrix}
A Z_{11} & A Z_{12} \\ 
B Z_{11} & B Z_{12}
\end{pmatrix} = Q
\begin{pmatrix}
D & 0 \\ 
0 & 0 \\
0 & 0
\end{pmatrix} \text{DOPIŠI MEJE!,} \label{eq:2}
\end{equation}
iz česar sledi:
$$
\begin{pmatrix}
A Z_{11} \\ 
B Z_{11}
\end{pmatrix} = Q
\begin{pmatrix}
D \\ 
0
\end{pmatrix}
$$ in v kolikor še matriko $Q$ razdelimo na podmatrike na naslednji način:
$$ Q = 
\begin{pmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{pmatrix} \text{,}
$$ kjer je matrika $Q_{11} \in \mathbb{R}^{k \times k}$, matrika $Q_{12} \in \mathbb{R}^{k \times (p+n-k)}$, matrika $Q_{21} \in \mathbb{R}^{ (p+n-k) \times k}$ in matrika matrika $Q_{22} \in \mathbb{R}^{ (p+n-k) \times (p+n-k)}$, ugotovimo, da je:
$$Q
\begin{pmatrix}
D \\ 
0
\end{pmatrix} = 
\begin{pmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{pmatrix}
\begin{pmatrix}
D \\ 
0
\end{pmatrix} =
\begin{pmatrix}
Q_{11} D \\ 
Q_{21} D
\end{pmatrix} \text{.}
$$
Iz tega neposredno sledi enakost:
$$
A Z_{11} = Q_{11} D \implies A Z_{11} D^{-1} = Q_{11} =: A_1 \text{,}
$$ kjer dodatno definiramo matriko $A_1$
in enakost:
$$
B Z_{11} = Q_{21} D \implies B Z_{11} D^{-1} = Q_{21} =: B_1 \text{,}
$$ kjer dodatno definiramo matriko $B_1$.

Ker je matrika $Q$ ortogonalna dodatno velja $A_1^TA_1 + B_1^TB_1 = I_k$, kjer je $I_k$ identična matrika dimzije $k \times k$. To enačbo lahko dobimo tako, da razpišemo spodnjo enačbo:
\begin{gather*}
Q^T Q = 
\begin{pmatrix}
Q_{11}^T & Q_{21}^T \\ 
Q_{12}^T & Q_{22}^T
\end{pmatrix}
\begin{pmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{pmatrix} =
\begin{pmatrix}
Q_{11}^T Q_{11} + Q_{21}^T Q_{21} & Q_{11}^T Q_{12} + Q_{21}^T Q_{22} \\ 
Q_{12}^T Q_{11} + Q_{22}^T Q_{21} & Q_{12}^T Q_{12} + Q_{22}^T Q_{22}
\end{pmatrix} \\ =
\begin{pmatrix}
A_1^T A_1 + B_1^T B_1 & Q_{11}^T Q_{12} + Q_{21}^T Q_{22} \\ 
Q_{12}^T Q_{11} + Q_{22}^T Q_{21} & Q_{12}^T Q_{12} + Q_{22}^T Q_{22}
\end{pmatrix}  = I =
\begin{pmatrix}
I_k & 0\\ 
0 & I_{p+n-k}
\end{pmatrix} \text{.}
\end{gather*}

Sedaj singularni razcep naredimo na matriki $B_1$. Za matriko $B_1$ vemo, da ima isti rang kot matrika $B$, saj velja, da je matrika $Z_{11}$ polnega ranga (je namreč podmatrika ortogonalne matrike $Z$) in vemo, da je matrika $D^{-1}$ polnega ranga. Označimo: $r = rang(B) = rang(B_1)$. Iz singularnega razcepa za matriko $B_1$ dobimo ortogonalni matriki $V \in \mathbb{R}^{ n \times n}$ in $Z_2 \in \mathbb{R}^{ k \times k}$, da velja:
\begin{equation}
V^T B_q Z_2 = diag(\beta_1, \ldots, \beta_p) \text{,} 
\end{equation} kjer je $p = min\{n, k\}$ in velja 
$ \beta_1 \geq \beta_2 \geq \ldots \geq \beta_r > \beta_{r+1} = \ldots = 0 $. Enačbo lahko tudi obrnemo in velja $B_1 = B Z_{11} D^{-1}$

Iz enačbe \textit{\eqref{eq:2}} sledi, da velja:
$$
B Z_{12} = 0^{n \times k} \text{,}
$$
iz česar pa sledi, da je $\beta_{p+1} = \ldots = \beta_{q} = 0$.
Opazimo, da velja tudi:
\begin{gather*}
V^T B Z
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix} = 
\begin{pmatrix}
V^T B Z_{11} D^{-1} Z_2 & 0\\ 
0 & V^T B Z_{12} I_{m-k}
\end{pmatrix} \\ =
\begin{pmatrix}
diag(\beta_1, \ldots, \beta_p) & 0\\ 
0 & diag(\beta_{p+1}, \ldots, \beta_q)
\end{pmatrix} \text{.}
\end{gather*}

Podobno bomo pokazali tudi za matriko $A$. Za $G$ definirajmo matriko, ki jo dobimo s preoblikovanjem enačbe \textit{\eqref{eq:3}}:
$$ G := B_1 Z_2 = V diag(\beta_1, \ldots, \beta_p) \text{.}
$$
Opazimo, da so stolpci matrike $A_1 Z_2$ medsebojno ortogonalni, ker velja:
\begin{gather*}
(A_1 Z_2)^T (A_1 Z_2) = Z_2^T A_1^T A_1 Z_2 \\
= Z_2^T (I_{k} - B_1^T B_1) Z_2 = Z_2^T Z_2 -  Z_2^T B_1^T B_1 Z_2 \\
= I_k - G^T G = I_k - diag(\beta_1, \ldots, \beta_p) V^T V diag(\beta_1, \ldots, \beta_p) \\
= diag(1 - \beta_1^2, \ldots,1 - \beta_p^2) \text{.}
\end{gather*}
Tako se v diagonalne vrednosti preslikajo dolžine posameznih stolpcev (skalarni produkti enakih stolpcev), vsi ostali skalarni produkti pa so enaki 0. Če za matriko A dodatno definiramo $\alpha_i = 0 \text{za} i = k+1, \ldots, n$ lahko vidimo, da iz $A_1 = A Z_{11} D^{-1}$ sledi:
$$ U^T A_1 Z_2 = U^T A Z_{11} D^{-1} Z_2 = diag(\alpha_1, \ldots, \alpha_k) \text{.}
$$ 
Podobno, kot za matriko $B$ tu lahko pokažemo:
\begin{gather*}
U^T A Z_1 
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix} \\
=
\begin{pmatrix}
U^T A Z_{11} D^{-1} Z_2 & 0\\ 
0 & U^T A Z_{12} I_{m-k}
\end{pmatrix} =
\begin{pmatrix}
diag(\alpha_1, \ldots, \alpha_k) & 0\\ 
0 & diag(\alpha_{k+1}, \ldots, \alpha_n)
\end{pmatrix} \text{,}
\end{gather*}
kjer smo pri prehodu v zadnjo vrstico ponovno uporabili enačbo \textit{\eqref{eq:2}}. Sedaj definiramo matriko $X$ na sledeči način:
$$ X := Z_1 
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix}
$$
\end{proof}

Problem tega izreka ja, da se ga ne da uporabiti, kadar dimenzije matrike $K_A$ niso ustrezne. Zaradi tega pretirano zavezujočega pogoja se odločita C.C. Paige in M.A. Saunders ta posplošeni singularni izrek še dodatno posplošiti. Tako dobimo naslednji izrek:
\begin{izrek}[Posplošeni singularni izrek (Paige in Saunders):]
\label{izrek:GSVD} Naj bosta dani matriki $K_A \in \mathbb{R}^{p \times m}$ in $K_B \in \mathbb{R}^{n \times m}$. Potem za $K = \left(\begin{array}{c} K_A \\ K_B \end{array}\right)$ in $t = rang(K)$ obstajajo ortogonalne matrike $U \in \mathbb{R}^{p \times p}$, $V \in \mathbb{R}^{n \times n}$, $W \in \mathbb{R}^{t \times t}$ in $Q \in \mathbb{R}^{m \times m}$, da velja: 
$$U^T K_A Q = \Sigma_A  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right) \hspace{4mm} \text{in} \hspace{4mm} V^T K_B Q = \Sigma_B  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right),$$ kjer je 
$$\Sigma_A = \begin{pmatrix} 
I_A &  & \\
 & D_A & \\
 & & 0_A  
\end{pmatrix} \hspace{4mm} \text{in} \hspace{4mm}
\Sigma_B = \begin{pmatrix} 
0_B &  & \\
 & D_B & \\
 & & I_B  
\end{pmatrix}.$$ 
$R \in \mathbb{R}^{t \times t}$ je nesingularna matrika, matriki $I_A \in \mathbb{R}^{r \times r}$ in $I_B \in \mathbb{R}^{(t-r-s) \times (t-r-s)}$ identični matriki, kjer je 
$$r = rang(K) - rang(K_B) \hspace{4mm} \text{in} \hspace{4mm} s = rang(K_A) + rang(K_B) - rang(K),$$
$0_A \in \mathbb{R}^{(p-r-s) \times (t-r-s)}$ in $0_B \in \mathbb{R}^{(n-t+r) \times r}$ ničelni matriki, ki imata lahko tudi ničelno število vrstic ali stolpcev, matriki
$D_A = diag(\alpha_{r+1},..., \alpha_{r+s})$ in $D_B = diag(\beta_{r+1},..., \beta_{r+s})$ pa diagonalni matriki, ki zadoščata pogoju:
$$1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0 \hspace{4mm} \text{in} \hspace{4mm} 0 < \beta_{r+1} \leq \ldots \leq \beta_{r+s} < 1$$
pri $\alpha_i^2 + \beta_i^2 = 1$ za $ i = r+1,\ldots, r+s$
\end{izrek}
\begin{proof}
Dovolj je, če ta izrek dokažemo za vsa kompleksna števila. Iz dejstva, da je množica realnih števil ($\mathbb{R}$) podmnožica množice kompleksnih števil ($\mathbb{C}$), sledi, da potem ta izrek velja tudi za vsa realna števila. Definirajmo matriko $K$, ki je sestavljena kot matrika sestavljena iz $K_A$ in $K_B$, torej $$K:= \begin{pmatrix} 
K_A \\
K_B 
\end{pmatrix}.$$
Na zgoraj definirani matriki $K$ lahko sedaj naredimo singularni razcep. Tako vemo, da za matriko $K$ obstajata unitarni matriki $P \in \mathbb{C}^{(m+p) \times (m+p)}$ in $Q \in \mathbb{C}^{n \times n}$, da velja $$ P^H K Q = 
\begin{pmatrix} 
R & 0 \\
0 & 0 
\end{pmatrix}$$, kjer ima matrika $R$ enak rang kot matrika $K$. Matriki $Q$ in $P$ sedaj ločimo na sledeče podmatrike:
$$ Q = 
\begin{pmatrix} 
Q_1, & Q_2
\end{pmatrix}
\hspace{4mm} \text{in} \hspace{4mm}
P = 
\begin{pmatrix} 
P_1, & P_2
\end{pmatrix}
=
\begin{pmatrix} 
P_{11} & P_{12} \\
P_{21} & P_{22} 
\end{pmatrix},
$$
kjer je matrika $Q_1 \in \mathbb{C}^{m \times t}$ sestavljena iz prvih $k$ stolpcev matrike $Q$, matrika $P_1 \in \mathbb{C}^{(p+n) \times t}$ pa izprvih $t$ stolpcev matrike $P$ in njena podmatrika $P_{11} \in \mathbb{C}^{p \times t}$ pa iz prvih m vrstic matrike $P_1$.
Vemo, da ker je matrika $P$ unitarna matrika, velja $\norm{P}_2 \leq 1$ in posledično velja še $\norm{P_{11}}_2 \leq \norm{P_{1}}_2 \leq \norm{P}_2 \leq 1$. Iz izreka iz numeričnih metod velja, da posledično nobena lastna vrednost matrike $P_{11}$ ni večja od $1$.

Singularni razcep podobno kot na matriki $K$ naredimo tudi na matriki $P_{11}$. Tako dobimo takšni matriki $U \in \mathbb{C}^{p \times p}$ in $W \in \mathbb{C}^{ \times t}$, da velja $$ U^H P_{11} W = \Sigma_A,$$ kjer je $$\Sigma_A = 
\begin{pmatrix} 
0_B &  & \\
 & D_B & \\
 & & I_B  
\end{pmatrix},$$ kjer je matrika $D_B$ diagonalna matrika z diagonalnimi vrednostmi $\alpha_{r+1},..., \alpha_{r+s},$ za katere velja $1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0.$

(Spodnji del je vprašljiv? \textbf{-- DOPOLNI!})

Na matriki $P_{21}$ uporabimo čudežni razcep (s Householderjevimi zrcaljenji) in tako dobimo matriko $V \in \mathbb{C}^{n \times n}$, da velja $$ V^H P_{21} W = L = (\ell_{ij})_{i, j} =
\begin{pmatrix} 
0 &  \\
& L_1
\end{pmatrix}
,$$ kjer je matrika $L_1$ spodnjetrikotna z diagonalnimi elementi večjimi od $0$.
Opazimo lahko, da velja spodnja enakost
$$
\begin{pmatrix} 
U^T & 0 \\
0 & V^T
\end{pmatrix}
\begin{bmatrix}
P_{11} \\
P_{21}
\end{bmatrix} W = 
\begin{bmatrix}
U^H P_{11} W \\
V^H P_{21} W
\end{bmatrix} =
\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}.
$$
Zgornja matrika $\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}$ je unitarna, saj je produkt unitarnih matrik. Posledično so njeni stolpci ortonormirani.
\end{proof}


% DODATNO - IZPIS MATIK Z DIMENZIJAMI
%\begin{tikzpicture}[decoration=brace]
%    \matrix (m) [matrix of math nodes,left delimiter=[,right delimiter={]}] {
%        a & b & c & d \\
%        e & f & g & h \\
%        i & j & k & l \\
%        m & n & o & p \\
%    };
%    \draw[decorate,transform canvas={xshift=-1.5em},thick] (m-3-1.south west) -- node[left=2pt] {$A$} (m-1-1.north west);
%    \draw[decorate,transform canvas={yshift=0.5em},thick] (m-1-2.north west) -- node[above=2pt] {$R$} (m-1-4.north east);
%\end{tikzpicture}

\section{Matematična rešitev problema}
\subsection{Posplošitev linearne diskriminantne analize}

\subsection{Posplošitev maksimizacijskega kriterija $sled((S_W^Y)^{-1}S_B^Y)$}

\section{Algoritem}

\section{Zaključek}

\section{Viri}

\end{document}