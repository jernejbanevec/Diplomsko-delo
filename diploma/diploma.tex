% POZOR! - Pazi, da imaš nastavljeno vrednost "pdfLaTeX" v zgornjem okencu
\documentclass[mat1]{article}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{arydshln}
\newtheorem{izrek}{Izrek}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}

%za x piši \times
% \in \mathbb{R}^{m \times n}

% Kjer so nejasnosti, dodaj znak oz. opozorilo za dopolnitev ali nejasnost
% \textbf{-- NEJASNOST!}
% \textbf{-- DOPOLNI!}

\begin{document}

\section{Uvod}

\subsection{Uvod}

\textbf{-- Ta odstavek je potrebno dopolniti!}

Diskriminantna analiza se že dolga leta uporablja za določevanje lastnosti, ki poudarjajo razlike med razredi. Definirana je kot optimizacijski problem, ki vključuje kovariančne matrike, ki predstavljajo razpršenost podatkov znotraj posameznega razreda in razpršenost oziroma ločenost posameznih razredov. Diskriminantna analiza pa sama po sebi zahteva, da je ena od teh kovariančnih matrik nesingularna, kar omejuje njeno uporabo na matrikah določenih dimenzij. V nadaljevanju tako preučimo več različnih optimizacijskih kriterijev in poskušamo njihovo uporabo razširiti na vse matrike z uporabo posplošenega singularnega razcepa. Na ta način se izognemo pogoju nesingularnosti, ki ga zahteva diskriminantna analiza. Tako pridemo do posplošene diskriminantne analize, ki jo lahko uporabimo tudi, kadar je ena matrika singularna (v nadaljevanju lahko vidimo, da je matrika singularna, kadar je velikost vzorca manjša, kot pa je dimenzija posamezne meritve. V nadaljevanju bomo testirali učinkovitost posplošene diskriminantne analize in jo, kjer bo to mogoče, primerjali tudi z običajno diskriminantno analizo.

\subsection{Matematični uvod}

Cilj diskriminantne analize je združiti lastnosti originalnih podatkov na način, ki kar najučinkoviteje ločuje med razredi, v katerih so podatki. Pri takšnem združevanju lastnosti podatkov se dimenzija teh podatkov zmanjša tako, da se struktura teh podatkov in določenih razredov kar najbolje ohrani.

Predpostavimo, da so podatki zloženi v matriko $A \in \mathbb{R}^{m \times n}$, kjer $m$ predstavlja dimenzijo posamezne meritve, $n$ pa predstavlja število meritev oz. podatkov. Denimo, da so podatki v matriki $A$ iz $k$ različnih razredov. Tako so stolpci $a_1, a_2, \ldots, a_n$ matrike $A$ združeni v $k$ podmatrik, ki predstavljajo razrede, v katerih so podatki:
$$ A = 
\begin{pmatrix}
A_1, & A_2, & \ldots, & A_k
\end{pmatrix} \text{,}
\hspace{2mm} \text{kjer} \hspace{2mm} A_i \in \mathbb{R}^{m \times n_i} \text{.}
$$ 
Tu število $n_i$ predstavlja moč indeksne množice razreda $i$. To indeksno množico razreda $i$ označimo z $N_i$. Očitno velja tudi $$
\sum_{i=1}^{k}n_i = n \text{.}$$
Matriko $A$ lahko poleg razdelitve na podmatrike razdelimo tudi na stolpce. Matrika $A = \left( a_{i ,j} \right)  \in \mathbb{R}^{m \times n}$ je tako sestavljena iz $n$ posameznih stolpcev, kjer $i$-ti stolpec označimo z $a_i$:
$$ a_i =
\begin{bmatrix}
a_{1, i} \\
a_{2, i} \\
\vdots \\
a_{m, i}
\end{bmatrix}
\text{.}
$$

Cilj diskriminantne analize je najti linearno preslikavo iz $\mathbb{R}^m$ v $\mathbb{R}^\ell$, ki v novem prostoru kar najbolje poudari razrede, v katerih so podatki. Tu navadno velja $\ell \leq m - 1$, torej da je prostor, v katerega ta linearna preslikava slika, manjdimenzionalen kot prvotni prostor. To iskano linearno preslikavo predstavimo z matriko $G^T \in \mathbb{R}^{\ell \times m}$.
Za preslikavo $G^T$ torej velja $$G^T : \mathbb{R}^m \rightarrow \mathbb{R}^\ell \text{.}$$ Torej preslikava $G^T$ nek $m$-dimenzionalen vektor preslika v nov vektor v $\ell$-dimezionalnem prostoru, v katerem so razredi podatkov poudarjeni, razpršenost podatkov znotraj razredov je zmanjšana, razlike med razredi pa so povečane.

Za nadaljnje izračune moramo definirati tudi centroid $i$-tega razreda, ki je izračunan kot povprečje stolpcev v $i$-tem razredu, 
$$c^{(i)} = \frac{1}{n_i} \sum_{j \in N_i} a_j \text{,}
$$
in centroid celotnih podatkov, ki je izračunan kot povprečje vseh stolpcev, to je
$$c = \frac{1}{n} \sum_{j = 1}^{n} a_j \text{.}
$$

Razpršenost podatkov znotraj posameznih razredov, razpršenost vseh podatkov ter razpršenost oziroma razlike med razredi je smiselno predstaviti s pomočjo matrik. Zato v nadaljevanju definiramo matriko
$$S_W = \sum_{i = 1}^{k} \sum_{j \in N_i}(a_j - c^{(i)})(a_j - c^{(i)})^T\text{,}$$
ki predstavlja matriko razpršenosti podatkov znotraj razredov, matriko
$$S_B = \sum_{i = 1}^{k} \sum_{j \in N_i} ( c^{(i)} - c)( c^{(i)} - c)^T = \sum_{i = 1}^{k} n_i ( c^{(i)} - c)( c^{(i)} - c)^T \text{,}$$
ki predstavlja matriko razpršenosti oziroma razlik med razredi in matriko
$$S_M = \sum_{j = 1}^{n} (a_j - c)(a_j - c)^T \text{,}$$
ki pradstavlja matriko celotne razpršenosti podatkov. Vse tri matrike so velikosti $m \times m$.
S pomočjo preslikave $G^T$ pa jih preslikamo v matrike velikosti $\ell \times \ell$ na sledeč način:
$$ S_{W}^{\ell} = G^T S_W G \text{,} \hspace{2mm} S_{B}^{\ell} = G^T S_B G \text{,} \hspace{2mm} S_{M}^{\ell} = G^T S_M G \text{.}
$$
%Med zgoraj definiranimi matrikami velja tudi enakost:
%$S_M = S_W + S_B \text{.}$ \textbf{-- DOKAŽI (pozneje, ko vpelješ še matrike H)}

Iz danih matrik razpršenosti podatkov bi radi tvorili kriterij kvalitete strukture razredov. Kriterij kvalitete strukture razredov mora imeti visoko vrednost, kadar so razredi, v katerih so podatki, strnjeni in dobro ločeni med seboj. Opazimo lahko, da $sled(S_W)$ predstavlja, kako skupaj so si podatki v posameznem razredu, saj velja
\begin{gather*} 
sled(S_W) = \sum_{t=1}^{m} \left( \sum_{i = 1}^{k} \sum_{j \in N_i}(a_{t, j} - c_t^{(i)})^2 \right)
= \sum_{i = 1}^{k} \sum_{j \in N_i} \left( \sum_{t=1}^{m} (a_{t, j} - c_t^{(i)})^2 \right) \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ a_j - c^{(i)}}_2^2 \text{.}
\end{gather*}
Podobno $sled(S_B)$ predstavlja ločenost med razredi, saj velja
\begin{gather*} 
sled(S_B) = \sum_{t=1}^{m} \left( \sum_{i = 1}^{k} \sum_{j \in N_i}(c_t^{(i)} - c_t)^2\right)
= \sum_{i = 1}^{k} \sum_{j \in N_i} \left( \sum_{t=1}^{m} (c_t^{(i)} - c_t)^2 \right) \\
= \sum_{i = 1}^{k} \sum_{j \in N_i} \norm{ c^{(i)} - c}_2^2 
= \sum_{i = 1}^{k} n_i \norm{ c^{(i)} - c}_2^2
\text{.}
\end{gather*}

Optimalna preslikava $G^T$ tako maksimizira $sled(S_{B}^{\ell})$ in minimizira $sled(S_{W}^{\ell})$. Smiselen maksimizacijski kriterij se tako zdi $$sled( G^T S_B G) / sled( G^T S_W G) \text{,}$$ ki pa ga zaradi lažjega računanja aproksimiramo kar s kriterijem $$sled((S_W^\ell)^{-1}S_B^\ell) \text{.}$$
%\textbf{-- NEJASNOST!}
% Pojasnitev prvega kriterija: Preslikana matrika S_B mora imeti čim manjšo vsoto lastnih vrednosti, preslikana matrika S_W pa čim večjo.
% Pojasnitev aproksimacijskega kriterija: Skalarni produkt lastnih vrednosti preslikanih matrik mora biti čim manjši - to je pribljižno enako

Kljub temu, da je ta optimizacijski kriterij lažje izračunljiv ima svoje pomanjkljivosti. Opazimo lahko, da kriterija ne moremo uporabiti, ko je matrika $S_W^\ell$ singularna, torej kadar je njena determinanta enaka 0. 

\textbf{-- Zelo narobe, razmisli za rešitev! --}
%Ta del mogoče sploh ne velja!!

Ker pa za determinanto matrike velja
$$ \det(S_W^\ell) = \det(G^T S_W G) = \det(G^T) \cdot \det(S_W) \cdot \det(G) \text{,}
$$
je $\det(S_W^\ell)$ enaka 0 kadar je $\det(S_W)$ enaka 0, torej kadar je matrika $S_W$ singularna. 

\textbf{--}

Do te situacije pa lahko pride kar precej pogosto. Matrika $S_W \in  \mathbb{R}^{m \times m}$ je singularna namreč v vseh primerih, ko za matriko $A \in  \mathbb{R}^{m \times n}$ velja $m > n$, saj je potem po definiciji sestavljena kot vsota $n$ matrik z rangom $1$. $m \times m$ dimenzionalna matrika $S_W$ ima tako rang manjši ali enak $n$, iz česar sledi, da je njena determinanta enaka 0.
Na primer, do tega problema pride v primeru, ko je pridobivanje podatkov drago oz. zahtevno in so pridobljeni podatki visokih dimenzij (dimenzija posameznega podatka je večja od števila vseh pridobljenih podatkov).

Obstaja več načinov, kako aplicirati diskriminantno analizo na matriki $A \in \mathbb{R}^{m \times n}$ z $m > n$. V grobem jih ločimo na tiste, kjer dimenzijo podatkov zmanjšamo v dveh korakih, in na tiste, kjer dimenzijo podatkov zmanjšamo v enem koraku. Pri prvem načinu se faza diskriminante analize nadaljuje v fazo, v kateri zanemarimo oblike posameznih razredov. Najpopularnejša metoda za prvi del tega procesa je zmanjšanje ranga matrike s pomočjo singularnega razcepa. To je tudi glavno orodje metode imenovane metoda glavnih komponent. Kakorkoli, celotna predstava dvostopenjskih načinov je precej občutljiva na zmanjšanje dimenzije v prvi fazi. V diplomskem delu se bomo osredotočili na način, ki posploši diskriminantno analizo tako, da teoretično optimalno zmanjša dimenzijo podatkov brez da bi uvedel dodaten korak. V ta namen bomo obravnavali kriterij 
$$ sled((S_2^\ell)^{-1} S_1^\ell) \hspace{1mm} \text{,}
$$
kjer matriki $S_2$ in $S_1$ predstavljata poljubno matriko izmed $S_W$, $S_B$ in $S_M$.  Kadar je matrika $S_2$ nesingularna, klasična diskriminantna analiza predstavi svojo rešitev s pomočjo posplošenega problema lastnih vrednosti. S prestrukturiranjem problema tako, da uporabimo posplošeni singularni razcep, pa lahko razširimo uporabnost diskriminantne analize tudi na primer, ko je matrika $S_2$ singularna.

\section{Matematična priprava - posplošeni singularni razcep}
Originalna definicija posplošenega singularnega razcepa (Van Loan) je sledeča.

% ZAČETEK PRVEGA IZREKA
\begin{izrek}[Posplošeni singularni izrek (Van Loan)]
\label{izrek:SVD} Za matriki $K_A \in \mathbb{R}^{p \times m}$ z $p \geq m$ in $K_B \in \mathbb{R}^{n \times m}$ obstajata ortogonalni matriki $U \in \mathbb{R}^{p \times p}$ in $V \in \mathbb{R}^{n \times n}$ ter nesingularna matrika $X \in \mathbb{R}^{m \times m}$, da velja 
$$ U^T K_A X = 
% !!DEFINICIJA PRVE MATRIKE!!
\begin{bmatrix}
\begin{matrix}
\alpha_1 & & \\
 & \ddots & \\
 & & \alpha_m
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p-m, m}
\end{bmatrix} 
% !!KONEC DEFINICIJE PRVE MATRIKE!!
\hspace{2mm} \text{in} \hspace{2mm}
 V^T K_B X = 
\Sigma_{B_q} \text{,}
$$ kjer je $q = min(n,m) \text{,}$
% !!DEFINICIJA DRUGE MATRIKE!!
\begin{gather*}
\Sigma_{B_q} = 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_q
\end{matrix} & 0_{q, m - q}
 \\ \hdashline[2pt/2pt]
0_{n-q, q} & 0_{n-q, m-q}
\end{array} \right] \text{,} 
\end{gather*} %\hspace{2mm}
% !!KONEC DEFINICIJE DRUGE MATRIKE!!
$
\alpha_i \geq 0 \hspace{2mm} \text{za} \hspace{2mm}
 1 \leq i \leq m \text{,} \hspace{2mm}
  \beta_i \geq 0 \hspace{2mm} \text{za} \hspace{2mm} 1 \leq i \leq q
\hspace{2mm} \text{in} \hspace{2mm} \beta_1 \geq \beta_2 \geq \ldots \geq \beta_q
\text{.}
$
\end{izrek}
\begin{proof}
% PREVERI SPODNJE PRETVORBE
% m_a = p
% m_b = n
% k = n
% k_1 = k
% n = m

Iz matrik $K_A$ in $K_B$ tvorimo združeno $(p+n)\times m$ matriko $K = \left(\begin{array}{c} K_A \\ K_B \end{array}\right)$, na kateri naredimo singularni razcep. Iz singularnega razcepa dobimo matriki $Q \in \mathbb{R}^{(p+n) \times (p+n)}$ in matriko $Z_1 \in \mathbb{R}^{m \times m}$, tako da velja 
\begin{equation}
Q^T \left(\begin{array}{c} K_A \\ K_B \end{array}\right) Z_1 = 
\begin{bmatrix}
\begin{matrix}
\gamma_1 & & \\
 & \ddots & \\
 & & \gamma_m
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p+n-m, m}
\end{bmatrix} 
\text{,}
\label{eq:1}
\end{equation}
kjer za
$
k = rang(K) \hspace{2mm} \text{velja} \hspace{2mm} 
\gamma_1 \geq \ldots \geq \gamma_k > \gamma_{k+1} = \ldots \gamma_m = 0\text{.} $

Matriko $Z_1$ razdelimo na dve matriki, matriko $Z_{11} \in \mathbb{R}^{m \times k}$, ki je sestavljena iz prvih $k$ stolpcev matrike $Z_1$ in matriko $Z_{12} \in \mathbb{R}^{m \times (m-k)}$, ki je sestavljena iz preostalih $m-k$ stolpcev matrike $Z_1$. Pišemo $$Z_1 = \left[ Z_{11} \hspace{2mm} Z_{12} \right] \text{.}$$

%, lahko vidimo, da velja
%$$ Q^T K \left(\begin{array}{c} Z_{11} | Z_{12} \end{array}\right) = 
%\begin{bmatrix}
%\begin{matrix}
%\gamma_1 & & & & & \\
% & \ddots & & & & \\
% & & \gamma_k & & & \\
% & & & \gamma_{k+1} & & \\
% & & & & \ddots & \\
% & & & & & \gamma_m 
%\end{matrix} \\ \hdashline[2pt/2pt]
%0_{p+n-m, m}
%\end{bmatrix}
%\text{.}
%$$
Po predpostavki velja $p \geq m$ in ker je očitno tudi $m \geq k$ sledi $p \geq m \geq k$. Sedaj definirajmo matriko 
$$ D := diag(\gamma_1,..., \gamma_k) \in \mathbb{R}^{k \times k} \text{.}
$$
Tako iz zgornje enačbe \textit{\eqref{eq:1}} dobimo
\begin{equation}
\begin{pmatrix}
K_A Z_{11} & K_A Z_{12} \\ 
K_B Z_{11} & K_B Z_{12}
\end{pmatrix} = Q
\begin{pmatrix}
D & 0_{k, m-k} \\ 
0_{p+n-k, k} & 0_{p+n-k, m-k} 
\end{pmatrix} \text{,} \label{eq:2}
\end{equation}
od koder sledi
$$
\begin{pmatrix}
K_A Z_{11} \\ 
K_B Z_{11}
\end{pmatrix} = Q
\begin{pmatrix}
D \\ 
0
\end{pmatrix} \text{.}
$$
V kolikor še matriko $Q$ razdelimo na podmatrike na naslednji način
$$ Q = 
\begin{pmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{pmatrix} \text{,}
$$ kjer je matrika $Q_{11} \in \mathbb{R}^{p \times k}$, matrika $Q_{12} \in \mathbb{R}^{p \times (p+n-k)}$, matrika $Q_{21} \in \mathbb{R}^{ n \times k}$ in matrika matrika $Q_{22} \in \mathbb{R}^{ n \times (p+n-k)}$, ugotovimo, da je
$$Q
\begin{pmatrix}
D \\ 
0
\end{pmatrix} = 
\begin{pmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{pmatrix}
\begin{pmatrix}
D \\ 
0
\end{pmatrix} =
\begin{pmatrix}
Q_{11} D \\ 
Q_{21} D
\end{pmatrix} \text{.}
$$
Iz tega neposredno sledi enakost
$$
K_A Z_{11} = Q_{11} D \implies K_A Z_{11} D^{-1} = Q_{11} =: K_{A_1}  \in \mathbb{R}^{p \times k} %\text{,}
$$
in enakost
$$
K_B Z_{11} = Q_{21} D \implies K_B Z_{11} D^{-1} = Q_{21} =: K_{B_1} \in \mathbb{R}^{ n \times k} \text{.}
$$
Sedaj singularni razcep naredimo na matriki $K_{B_1}$. Za matriko $K_{B_1}$ vemo, da ima isti rang kot matrika $K_B$, saj velja, da je matrika $Z_{11}$ polnega ranga (je namreč podmatrika ortogonalne matrike $Z$) in vemo, da je matrika $D^{-1}$ polnega ranga. Označimo $r = rang(K_B) = rang(K_{B_1})$. Iz singularnega razcepa za matriko $K_{B_1}$ dobimo ortogonalni matriki $V \in \mathbb{R}^{ n \times n}$ in $Z_2 \in \mathbb{R}^{ k \times k}$, da velja
\begin{equation}
V^T K_{B_1} Z_2 = \Sigma_{B_t}
 \text{,}  \label{eq:3}
\end{equation}
kjer je $t = min\{n, k\}$, 
$\Sigma_{B_t} = 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_t
\end{matrix} & 0_{t, k - t}
 \\ \hdashline[2pt/2pt]
0_{n-t, t} & 0_{n-t, k-t}
\end{array} \right]$
in velja 
$ \beta_1 \geq \beta_2 \geq \ldots \geq \beta_r > \beta_{r+1} = \ldots = \beta_t = 0 \text{.}$ 
\newline
Iz enačbe \textit{\eqref{eq:2}} sledi, da je
$$
K_B Z_{12} = 0_{n, m-k} \text{.}
$$
Opazimo, da velja tudi
\begin{gather*}
V^T K_B Z_1
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix} = 
V^T K_B \left[ Z_{11} \hspace{2mm} Z_{12} \right]
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix} = \\
V^T K_B
\begin{pmatrix}
 Z_{11} D^{-1} Z_2 & Z_{12} 
\end{pmatrix} =
\begin{pmatrix}
V^T K_B Z_{11} D^{-1} Z_2 & V^T K_B Z_{12} 
\end{pmatrix} = \\
\begin{pmatrix}
V^T K_{B_1} Z_2 & 0_{n, m-k} 
\end{pmatrix} =
\begin{pmatrix}
\Sigma_{B_t} & 0_{n, m-k} 
\end{pmatrix} = \\
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_t
\end{matrix} & 0_{t, m - t}
 \\ \hdashline[2pt/2pt]
0_{n-t, t} & 0_{n-t, m-t}
\end{array} \right]
\text{.}
\end{gather*}
Če za $q = \min\{n,m\}$ dodatno definiramo še $\beta_{t+1} = \ldots = \beta_{q} = 0$, dobimo ravno
\begin{gather*}
V^T K_B Z_1
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix} = 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1 & & \\
 & \ddots & \\
 & & \beta_q
\end{matrix} & 0_{q, m - q}
 \\ \hdashline[2pt/2pt]
0_{n-q, q} & 0_{n-q, m-q}
\end{array} \right]
\text{,}
\end{gather*}
kar je pa ravno matrika $\Sigma_{B_q}$ iz izreka. Matriko $X$ tako definiramo na sledeči način
$$ X := Z_1 
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix}
\text{.}$$

Dokazati pa moramo tudi, da zgornja matrika $X$ ustreza tudi enačbi iz izreka za matriko $K_A$, torej, da obstaja tudi taka matrika $U$, da velja
\begin{gather*}
 U^T K_A X = 
\begin{bmatrix}
\begin{matrix}
\alpha_1 & & \\
 & \ddots & \\
 & & \alpha_m
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p-m, m}
\end{bmatrix}
\text{.} 
\end{gather*}

Ker je matrika $Q$ ortogonalna, dodatno velja $K_{A_1}^TK_{A_1} + K_{B_1}^TK_{B_1} = I_k$, kjer je $I_k$ identična matrika dimenzije $k \times k$. To enakost lahko pokažemo tako, da razpišemo spodnjo enačbo
\begin{gather*}
Q^T Q = 
\begin{pmatrix}
Q_{11}^T & Q_{21}^T \\ 
Q_{12}^T & Q_{22}^T
\end{pmatrix}
\begin{pmatrix}
Q_{11} & Q_{12} \\ 
Q_{21} & Q_{22}
\end{pmatrix} =
\begin{pmatrix}
Q_{11}^T Q_{11} + Q_{21}^T Q_{21} & Q_{11}^T Q_{12} + Q_{21}^T Q_{22} \\ 
Q_{12}^T Q_{11} + Q_{22}^T Q_{21} & Q_{12}^T Q_{12} + Q_{22}^T Q_{22}
\end{pmatrix} \\ =
\begin{pmatrix}
K_{A_1}^TK_{A_1} + K_{B_1}^TK_{B_1} & Q_{11}^T Q_{12} + Q_{21}^T Q_{22} \\ 
Q_{12}^T Q_{11} + Q_{22}^T Q_{21} & Q_{12}^T Q_{12} + Q_{22}^T Q_{22}
\end{pmatrix}  = I =
\begin{pmatrix}
I_k & 0\\ 
0 & I_{p+n-k}
\end{pmatrix} \text{.}
\end{gather*}
Definirajmo matriko $G$, ki jo dobimo s preoblikovanjem enačbe \textit{\eqref{eq:3}}:
$$ G := K_{B_1} Z_2 = V \Sigma_{B_t} \in \mathbb{R}^{n \times k} \text{.}
$$

Za matriko $K_{A_1} Z_2$ izračunamo razširjen QR razcep, $ K_{A_1} Z_2 = U R$, kjer je $U \in \mathbb{R}^{p \times p}$ ortogonalna in $R \in \mathbb{R}^{p \times k}$ zgornja trapezna matrika. Tak razcep lahko naredimo na primer z uporabo Householderjevih zrcaljenj.

%STAR DEL!!!
%naredimo Householderjeva zrcaljenja: Dobimo matriko ortogonalno matriko $U^T \in \mathbb{R}^{p \times p}$, da velja
%$$ U^T K_{A_1} Z_2 = R \text{,}
%$$
%kjer je $R \in \mathbb{R}^{p \times k}$ matrika z zgornjo trapezno obliko. Ker je matrika $U$ ortogonalna, velja $ K_{A_1} Z_2 = U R$.

Opazimo lahko, da so stolpci matrike $K_{A_1} Z_2$ medsebojno ortogonalni, saj velja
\begin{gather*}
(K_{A_1} Z_2)^T (K_{A_1} Z_2) = Z_2^T K_{A_1}^T K_{A_1} Z_2 %\\
= Z_2^T (I_{k} - K_{B_1}^T K_{B_1}) Z_2 =
\\
Z_2^T Z_2 -  Z_2^T K_{B_1}^T K_{B_1} Z_2
= I_k - G^T G = I_k - \Sigma_{B_t}^T V^T V \Sigma_{B_t} 
\\
= I_k - 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\beta_1^2 & & \\
 & \ddots & \\
 & & \beta_t^2
\end{matrix} & 0_{t,k - t}
 \\ \hdashline[2pt/2pt]
0_{k - t, k - t} & 0_{t, k - t}
\end{array} \right] 
%\\
= 
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
1 - \beta_1^2 & & \\
 & \ddots & \\
 & & 1 - \beta_t^2
\end{matrix} & 0_{t,k - t}
 \\ \hdashline[2pt/2pt]
0_{k - t, k - t} & I_{t, k - t}
\end{array} \right]  
\\ 
= diag(1-\beta_1^2, \ldots, 1-\beta_k^2) \text{,}
\end{gather*}
kjer smo dodatno definirali še $\beta_{t+1} = \ldots = \beta_{k} = 0$. 
Iz tega sledi, da je matrika $R$ oblike 
$$ R = 
\begin{bmatrix}
\begin{matrix}
\sqrt{1 - \beta_1^2} & & \\
 & \ddots & \\
 & & \sqrt{1 - \beta_k^2}
\end{matrix} \\ \hdashline[2pt/2pt]
0_{p-k, k}
\end{bmatrix} \text{,}
$$ saj velja 
$$
(K_{A_1} Z_2)^T (K_{A_1} Z_2) = R^T U^T U R = R^T R = diag(1-\beta_1^2, \ldots, 1-\beta_k^2) \text{.}
$$
Velja tudi
\begin{gather*}
U^T K_A Z_1
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix} = 
U^T K_A %\left(\begin{array}{c} Z_{11} | Z_{12} \end{array}\right)
\left[Z_{11} \hspace{2mm} Z_{12} \right]
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix} = \\
U^T K_A
\begin{pmatrix}
 Z_{11} D^{-1} Z_2 & Z_{12} 
\end{pmatrix} =
\begin{pmatrix}
U^T K_A Z_{11} D^{-1} Z_2 & U^T K_A Z_{12} 
\end{pmatrix} = \\
\begin{pmatrix}
U^T K_{A_1} Z_2 & 0_{p, m-k} 
\end{pmatrix} =
\begin{pmatrix}
U^T U R & 0_{p, m-k}
\end{pmatrix} =
\begin{pmatrix}
R & 0_{p, m-k} 
\end{pmatrix} = \\
\left[
\begin{array}{c;{2pt/2pt}c}
\begin{matrix}
\alpha_i & & \\
 & \ddots & \\
 & & \alpha_k
\end{matrix} & 0_{k, m-k}
 \\ \hdashline[2pt/2pt]
0_{p-k, k} & 0_{p-k, m-k}
\end{array} \right] \text{,}
\end{gather*}
kjer smo dodatno definirali $\alpha_i = \sqrt{1-\beta_i^2}$ za $i = 1, \ldots, k$ in $\alpha_{k+1} = \ldots = \alpha_m$.
S tem smo pokazali, da matrika $$ X = Z_1 
\begin{pmatrix}
D^{-1} Z_2 & 0\\ 
0 & I_{m-k}
\end{pmatrix}
$$
zadošča tako razcepu matrike $K_A$ kot tudi razcepu matrike $K_B$ iz izreka in s tem dokazali izrek.
\end{proof}

Problem tega izreka pa je, da se ga ne da uporabiti, kadar dimenzije matrike $K_A$ niso ustrezne. Zaradi tega pretirano zavezujočega pogoja se odločita C.C. Paige in M.A. Saunders ta posplošeni singularni izrek še dodatno posplošiti. Tako dobimo naslednji izrek:
% !!!!!! Druga posplošitev Singularnega razcepa - IZREK !!!!!
\begin{izrek}[Posplošeni singularni razcep (Paige in Saunders)]
\label{izrek:GSVD} Naj bosta dani matriki $K_A \in \mathbb{R}^{p \times m}$ in $K_B \in \mathbb{R}^{n \times m}$. Potem za $K = \left(\begin{array}{c} K_A \\ K_B \end{array}\right)$ in $k = rang(K)$ obstajajo ortogonalne matrike $U \in \mathbb{R}^{p \times p}$, $V \in \mathbb{R}^{n \times n}$, $W \in \mathbb{R}^{k \times k}$ in $Q \in \mathbb{R}^{m \times m}$, da velja 
$$U^T K_A Q = \Sigma_A  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right) \hspace{4mm} \text{in} \hspace{4mm} V^T K_B Q = \Sigma_B  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right) \text{,}$$ kjer sta
$$\Sigma_A = \begin{pmatrix} 
I_A &  & \\
 & D_A & \\
 & & 0_A  
\end{pmatrix} \hspace{4mm} \text{in} \hspace{4mm}
\Sigma_B = \begin{pmatrix} 
0_B &  & \\
 & D_B & \\
 & & I_B  
\end{pmatrix} \text{,}$$ 
$R \in \mathbb{R}^{k \times k}$ je nesingularna matrika, matriki $I_A \in \mathbb{R}^{r \times r}$ in $I_B \in \mathbb{R}^{(k-r-s) \times (k-r-s)}$ identični matriki, kjer je 
$$r = rang(K) - rang(K_B) \hspace{4mm} \text{in} \hspace{4mm} s = rang(K_A) + rang(K_B) - rang(K),$$
$0_A \in \mathbb{R}^{(p-r-s) \times (k-r-s)}$ in $0_B \in \mathbb{R}^{(n-k+r) \times r}$ ničelni matriki, ki imata lahko tudi ničelno število vrstic ali stolpcev, matriki
$D_A = diag(\alpha_{r+1},..., \alpha_{r+s})$ in $D_B = diag(\beta_{r+1},..., \beta_{r+s})$ pa diagonalni matriki, ki zadoščata pogoju
$$1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0 \hspace{4mm} \text{in} \hspace{4mm} 0 < \beta_{r+1} \leq \ldots \leq \beta_{r+s} < 1$$
pri $\alpha_i^2 + \beta_i^2 = 1$ za $ i = r+1,\ldots, r+s$.
\end{izrek}
% !!!!! Dokaz druge posplošitve sing. razcepa !!!!!!!!
\begin{proof}
Definiramo matriko $K \in \mathbb{R}^{(p+n) \times m}$, ki je sestavljena iz matrik $K_A$ in $K_B$ kot $$K:= \begin{pmatrix} 
K_A \\
K_B 
\end{pmatrix}.$$
Z $k$ označimo rang te matrike $K$, torej $k = rang(K)$. Na zgoraj definirani matriki $K$ lahko sedaj naredimo singularni razcep. Tako vemo, da za matriko $K$ obstajata ortogonalni matriki $P \in \mathbb{R}^{(n+p) \times (n+p)}$ in $Q \in \mathbb{R}^{m \times m}$, da velja 
\begin{equation} P K Q^T = 
\begin{pmatrix} 
R & 0_{k, m-k} \\
0_{n+p-k, k} & 0_{n+p-k, m-k} 
\end{pmatrix} \text{,} \label{eq:4}
\end{equation}
kjer je $R$ diagonalna matrika ranga $k$. Matriki $Q$ in $P$ razdelimo na podmatrike na sledeči način:
$$  Q = \left(\begin{array}{c} Q_1 | Q_2 \end{array}\right)
\hspace{4mm} \text{in} \hspace{4mm}
P = \left(\begin{array}{c} P_1 | P_2 \end{array}\right)
=
\begin{pmatrix} 
P_{11} & P_{12} \\
P_{21} & P_{22} 
\end{pmatrix},
$$
kjer je matrika $Q_1 \in \mathbb{R}^{m \times k}$ sestavljena iz prvih $k$ stolpcev matrike $Q$, matrika $Q_2 \in \mathbb{R}^{m \times (m-k)}$ pa iz preostalih $m-k$ stolpcev matrike $Q$, podmatrike matrike $P$ pa so sledečih oblik: $P_{11} \in \mathbb{R}^{m \times k}$, $P_{12} \in \mathbb{R}^{m \times (m+p-k)}$, $P_{21} \in \mathbb{R}^{p \times k}$ in $P_{22} \in \mathbb{R}^{p \times (m+p-k)}$.
%, matrika $P_1 \in \mathbb{R}^{(p+m) \times k}$ iz prvih $k$ stolpcev matrike $P$ in njena podmatrika $P_{11} \in \mathbb{C}^{p \times t}$ pa iz prvih m vrstic matrike $P_1$.

Ker je $P$ ortogonalna matrika, vemo, da velja $\norm{P}_2 \leq 1$ in posledično tudi $\norm{P_{11}}_2 \leq \norm{P_{1}}_2 \leq \norm{P}_2 \leq 1$. Posledično nobena singularna vrednost matrike $P_{11}$ ni večja od $1$. %Velja po izreku iz numeričnih metod, ne vem kako se kliče

Singularni razcep podobno kot na matriki $K$ naredimo tudi na matriki $P_{11}$. Tako dobimo takšni matriki $U \in \mathbb{R}^{m \times m}$ in $W \in \mathbb{R}^{k \times k}$, da velja $$ U^T P_{11} W = \Sigma_A \text{,}$$ kjer je $$\Sigma_A = 
\begin{pmatrix} 
I_{r, r} &  & \\
 & D_A & \\
 & & 0_A  
\end{pmatrix} \text{,}$$ kjer je $r$ geometrična večkratnost lastne vrednosti $1$, matrika $I_{r, r}$ identična matrika dimenzije $r \times r$, matrika $D_A =
\begin{pmatrix}
\alpha_{r+1} & & \\
 & \ddots & \\
 & & \alpha_{r+s}
\end{pmatrix}$ diagonalna matrika, kjer $r+s$ predstavlja rang matrike $P_{11}$ in $0_A \in \mathbb{R}^{(p-r-s) \times (k-r-s)}$ je ničelna matrika, ki ima lahko nič vrstic ali nič stolpcev.
% za katere velja $1 > \alpha_{r+1} \geq \ldots \geq \alpha_{r+s} > 0.$

Na matriki $P_{21}$ uporabimo \textbf{(-- DOPOLNI!)} razcep (s Householderjevimi zrcaljenji) in tako dobimo matriko $V \in \mathbb{R}^{n \times n}$, da velja $$ V^H P_{21} W = L = (\ell_{ij})_{i, j} =
\begin{pmatrix} 
0 & 0 \\
0 & L_1
\end{pmatrix} 
,$$ \textbf{(-- POVEJ KATERIH DIMENZIJ SO TE NIČELNE MATRIKE!)} kjer je matrika $L_1$ spodnjetrikotna z diagonalnimi elementi večjimi od $0$.
Opazimo lahko, da velja spodnja enakost
$$
\begin{pmatrix} 
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{pmatrix}
\begin{bmatrix}
P_{11} \\
P_{21}
\end{bmatrix} W = 
\begin{bmatrix}
U^T P_{11} W \\
V^TP_{21} W
\end{bmatrix} =
\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}.
$$
Zgornja matrika $\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}$ je ortogonalna, saj je produkt ortogonalnih matrik. To implicira obliko matrike $ \begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}$, torej
$$
\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix} =
\left(
\begin{array}{c;{2pt/2pt}c;{2pt/2pt}c}
I_{r,r} & & \\ 
\hdashline[2pt/2pt]
 & D_A & \\ 
\hdashline[2pt/2pt]
 & & 0_{m-r-s, k-r-s} \\ 
\hdashline[2pt/2pt]
0_{p-k+r, r} & & \\ 
\hdashline[2pt/2pt]
 & D_B & \\ 
\hdashline[2pt/2pt]
 & & I_{k-r-s, k-r-s}
\end{array} \right) 
%=
%\begin{pmatrix}
%I_{r, r} &  & \\
% & D_A & \\
% & & 0_{m-r-s, k-r-s} \\
%0_{p-k+r, r} &  & \\
% & D_B & \\
% & & I_{k-r-s, k-r-s}
%\end{pmatrix} 
\text{,}
$$ kjer je matrika $D_B$ diagonalna matrika 
$  D_B = 
\begin{pmatrix}
\beta_{r+1} & & \\
 & \ddots & \\
 & & \beta_{r+s}
\end{pmatrix}$.
Za matriko $L$ pa velja $L = \Sigma_B =
\begin{pmatrix}
0_{p-k+r, r} & & \\ 
 & D_B & \\ 
 & & I_{k-r-s, k-r-s}
\end{pmatrix}$.
Iz ortogonalnosti matrike $\begin{pmatrix}
\Sigma_A \\
L
\end{pmatrix}$ sledi tudi, da so njeni stolpci ortonormirani (torej tudi po meri enaki 1), iz česar sledi $\alpha_{r+i}^2 + \beta_{r+i}^2 = 1$ za $i = 1, \ldots, s$.

Iz enačbe  \textit{\eqref{eq:4}} sledi
\begin{equation}
%\begin{gather*}
\begin{pmatrix} 
K_A \\
K_B 
\end{pmatrix} Q =
\begin{pmatrix} 
P_{11} & P_{12} \\
P_{21} & P_{22} 
\end{pmatrix}
\begin{pmatrix} 
R & 0_{k, m-k} \\
0_{n+p-k, k} & 0_{n+p-k, m-k}  
\end{pmatrix} =
\begin{pmatrix} 
P_{11}R & 0_{m, m-k} \\
P_{21}R & 0_{p, m-k} 
\end{pmatrix} =
\begin{pmatrix} 
U \Sigma_A W^T R & 0_{m, m-k} \\
V \Sigma_B W^T R & 0_{p, m-k} 
\end{pmatrix} \text{,} \label{eq:5}
%\end{gather*}
\end{equation}
iz česar sledi
$$
P_{11} = U \Sigma_A W^T
\hspace{2mm} \text{in} \hspace{2mm}
P_{21} = V \Sigma_B W^T \text{.}
$$
Če začetno enačbo iz enačbe  \textit{\eqref{eq:5}} pomnožimo iz leve z matriko 
$
\begin{pmatrix}
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{pmatrix}
$ dobimo 
$$
\begin{pmatrix}
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{pmatrix}
\begin{pmatrix} 
K_A \\
K_B 
\end{pmatrix} Q =
\begin{pmatrix}
U^T & 0_{m, n} \\
0_{n, m} & V^T
\end{pmatrix}
\begin{pmatrix} 
U \Sigma_A W^T R & 0_{m, m-k} \\
V \Sigma_B W^T R & 0_{p, m-k} 
\end{pmatrix} \text{,}
$$
iz česar sledi
$$
U^T K_A Q = 
\Sigma_A  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right) \hspace{4mm} \text{in} \hspace{4mm} V^T K_B Q = \Sigma_B  \left(\begin{array}{cc} W^T R, & 0 \end{array}\right) \text{,}
$$
s čimer smo dokazali izrek.
\end{proof}

Iz posplošenega singularnega razcepa, ki sta ga definirala Paige in Saunders neposredno sledi Van Loanova posplošitev singularnega razcepa. 
S preoblikovanjem enačbe \textit{\eqref{eq:3}}, dobimo:
$$U^T K_A Q = \left(\begin{array}{cc} \Sigma_A, & 0 \end{array}\right)
\begin{pmatrix}
W^TR & 0 \\
0 & I 
\end{pmatrix} \text{.}$$
Inverz matrike
$\begin{pmatrix}
W^TR & 0 \\
0 & I 
\end{pmatrix} $
je kar matrika 
$\begin{pmatrix}
R^{-1} W & 0 \\
0 & I 
\end{pmatrix} \text{,}$
saj veljata obe enakosi iz definicije inverza, torej:
$$\begin{pmatrix}
R^{-1} W & 0 \\
0 & I 
\end{pmatrix}
\begin{pmatrix}
W^TR & 0 \\
0 & I 
\end{pmatrix}
= I \hspace{3mm}\text{in} \hspace{3mm}
\begin{pmatrix}
W^TR & 0 \\
0 & I 
\end{pmatrix}
\begin{pmatrix}
R^{-1} W & 0 \\
0 & I 
\end{pmatrix}
= I \text{.}$$
Iz enačbe \textit{\eqref{eq:3}} tako tu očitno sledi:
$$ U^T K_A Q
\begin{pmatrix}
R^{-1} W & 0 \\
0 & I 
\end{pmatrix}
=
\left(\begin{array}{cc} \Sigma_A, & 0 \end{array}\right) \text{.}
$$
Matriko $X$ definiramo kot:
$$ Q
\begin{pmatrix}
R^{-1} W & 0 \\
0 & I 
\end{pmatrix}
$$ in dobimo ravno Van Loanovo posplošitev razcepa:
$$ U^T K_A X = \left(\begin{array}{cc} \Sigma_A, & 0 \end{array}\right) \text{.}
$$
Podobno lahko pokažemo tudi za matriko $K_B$, za katero dobimo enačbo:
$$ U^T K_B X = \left(\begin{array}{cc} \Sigma_B, & 0 \end{array}\right) \text{.}
$$
Tako je definirana matrika $X$ ravno iskana matrika iz prve posplošitve singularnega razcepa in matriki $\left(\begin{array}{cc} \Sigma_A, & 0 \end{array}\right)$ in $ U^T K_B X = \left(\begin{array}{cc} \Sigma_B, & 0 \end{array}\right)$ ravno iskani diagonalni matriki.

Za nadaljnje delo definirajmo matrike
$$H_W := [A_1 - c^{(1)}e^{(1)^T}, \ldots, A_k - c^{(k)}e^{(k)^T}] \text{,}$$
$$H_B := [(c^{(1)} - c)e^{(1)^T}, \ldots,(c^{(k)} - c) e^{(k)^T}]$$
in
$$H_M := [a_1 - c, ..., a_n - c] = A - ce^T = H_W + H_B\text{,}$$
kjer velja:
$e^{(i)} = (1,\ldots, 1) ^T \in \mathbb{R}^{ n_i \times 1 }$ in $e =  (1,\ldots, 1) ^T \in \mathbb{R}^{ n \times 1 }$.

S pomočjo teh matrik lahko definiramo tudi matrike razpršenosti podatkov. Matriko $S_W$ lahko definiramo kot produkt matrike $H_W$ z njeno transponiranko, torej:
$$S_W = H_W H_W^T \text{,}$$
matriko $S_B$ lahko definiramo na podoben način:
$$S_B = H_B H_B^T \text{,}$$
prav tako pa tudi matrika $S_M$:
$$S_M = H_M H_M^T \text{.}$$

S pomočjo tega razcepa lahko tudi na drugačen način pokažemo, da je matrika $S_W$, kadar velja $m>n$, singularna. Razvidno je namreč, da je ta matrika $S_W$ definirana kot produkt dveh matrik, kjer je matrika $H_W$ dimenzije $m \times n$, matrika $H_W^T$ pa dimenzije $n \times m$. V kolikor velja $m>n$, sta tako ti dve matrike največ ranga $n$.
Ker pa za rang matrike velja, da je rang produkta dveh matrik navzgor omejen z manjšim izmed rangov teh dveh posameznih matrik ($rang(AB) \leq \min(rang(A), rang(B)$), je posledično tudi matrika $S_W$ največ ranga $n$ in torej očitno singularna.

\textbf{-- Tu mogoče dopiši glede matrik $K_A$ in $K_B$}

\section{Matematična rešitev problema}
V tem odstavku prikažemo uporabo posplošenega singularnega razcepa v namen razširjene uporabe posplošene diskriminantne analize.
 
\subsection{Optimizacija optimizacijskega kriterija $J_1 = sled(S_2^{-1} S_1)$ za nesingularno matriko $S_2$}
Tu izhajamo iz optimizacije optimizacijskega kriterija 
$$J_1(G) = sled((G^T S_2 G)^{-1} (G^T S_1 G))$$
z izbiro optimalne preslikave $G$, kjer sta matriki $S_1$ in $S_2$ izbrani izmed matrik $S_W$, $S_B$ in $S_M$. Ker je tu matrika $S_2$ nesingularna in je sestavljena kot produkt matrike in transponiranke te matrike je simetrično pozitivno definitna (posledično so vse lastne vrednosti te matrike večje ali enake 0). Za simetrično pozitivno definitno matriko pa obstaja razcep Choleskega; tako vemo da obstaja spodnjetrikotna matrika $V$ s pozitivnimi elementi na diagonali, da velja:
$$ S_2 = V V^T \text{.}
$$


\subsection{Posplošitev maksimizacijskega kriterija $sled((S_W^Y)^{-1}S_B^Y)$ za singularno matriko $S_2$}

\section{Algoritem}

\section{Zaključek}

\section{Priloge}

\begin{izrek}[Singularni razcep]
\label{izrek:SVD} Za vsako matriko $A \in \mathbb{R}^{m \times n}$, z lastnostjo $m \geq n$, obstaja singularni razcep 
$$A = U \Sigma V^T \text{,}$$
kjer sta $U \in \mathbb{R}^{m \times m}$ in $V \in \mathbb{R}^{n \times n}$ ortogonalni matriki, $\Sigma \in \mathbb{R}^{m \times n}$ je oblike
$$
\Sigma = 
\begin{pmatrix} 
\sigma_1 &  & \\
 & \ddots & \\
 & & \sigma_n  \\
 & & 
\end{pmatrix}$$
in $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_n \geq 0$ so singularne vrednosti matrike $A$.
\end{izrek}
\begin{proof}
Ker je $A^TA$ simetrična pozitivno semidefinitna matrika, so vse njene lastne vrednosti nenegativne. Označimo in uredimo jih kot
$$\sigma_1^2 \geq \sigma_2^2 \geq \ldots \geq \sigma_n^2 \geq 0 \text{.}$$
Ustrezni ortonormirani lastni vektorji $v_1, \ldots, v_n$ zadoščajo
$A^T A v_i = \sigma_i^2 v_i$ za $i = 1, \ldots, n \text{.}$
Naj bo $\sigma_r > 0$ in $\sigma_{r+1} = \cdots = \sigma_n = 0 \text{.}$
Matriko $V$ razdelimo na $V_1 = 
\left( v_1, \ldots, v_r
\right)$ in $V_2 = 
\left( v_{r+1}, \ldots, v_n
\right) \text{.}$ Iz
$$ (AV_2)^T (AV_2) = V_2^T A^T A V_2 = V_2^T \left( 0, \ldots, 0 \right) = 0
$$
sledi $AV_2 = 0 \text{.}$ Sedaj definiramo $u_i := \frac{1}{\sigma_i} Av_i$ za $i = 1, \ldots, r \text{.}$
Vekorji $u_1, \ldots, u_r$ so ortonormirani, saj je
$$ u_i^T u_j = \frac{1}{\sigma_i \sigma_j} v_i^T A^T A v_j = \frac{\sigma_j}{\sigma_i} v_i^T v_j = \delta_{ij} \text{,} \hspace{3mm} i,j = 1, \ldots, r \text{,}
$$
kjer smo v zapisu uproabili t.i. \emph{Kroneckerjev delta}, definiran z $\delta_{ij} = 1$ za $i = j$ in $\delta_{ij} = 0$ za $i \neq j$. Označimo 
$U_1 = 
\left( u_1 \hspace{2mm} \cdots \hspace{2mm} u_r
\right)$ in dopolnimo z $U_2 = 
\left( u_{r+1} \hspace{2mm} \cdots \hspace{2mm} u_n
\right)$, da je $ U = \left( U_1 \hspace{2mm} U_2 \right)$ ortogonalna matrika. Matrika $U^T A V$ ima obliko 
$$
U^T A V = 
\begin{pmatrix} 
U_1^T A V_1 & U_1^T A V_2 \\
U_2^T A V_1 & U_2^T A V_2
\end{pmatrix}
\text{.}
$$
Desna bloka sta zaradi $AV_2 = 0$ enaka $0$. Za $i = 1, \ldots, r$ in $k = 1, \ldots, m$ velja
$$ u_k^T A v_i = \sigma_i u_k^T u_i = \sigma_i \delta{ik} \text{,}
$$
torej $U_2^T A V_1 = 0$ in $U_1^T A V_1 = diag(\sigma_1, \ldots, \sigma_r) \text{.}$ Dobimo singularni razcep $A = U \Sigma V^T \text{,}$ kjer je $S = diag(\sigma_1, \ldots, \sigma_r)$ in 
$$ \Sigma = 
\begin{pmatrix} 
S & 0 \\
0 & 0
\end{pmatrix}
\text{.}$$

\end{proof}
V primeru, ko velja $n>m$, dobimo singularni razcep za $A \in \mathbb{R}^{m \times n}$ tako, da transponiramo singularni razcep $A^T$.
\section{Viri}

\end{document}